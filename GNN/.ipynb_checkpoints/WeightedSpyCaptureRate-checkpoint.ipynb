{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CNN2D:\n\tMissing key(s) in state_dict: \"conv1.0.weight\", \"conv1.0.bias\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv2.0.weight\", \"conv2.0.bias\", \"conv2.1.weight\", \"conv2.1.bias\", \"conv2.1.running_mean\", \"conv2.1.running_var\", \"conv3.0.weight\", \"conv3.0.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"module.conv1.0.weight\", \"module.conv1.0.bias\", \"module.conv1.1.weight\", \"module.conv1.1.bias\", \"module.conv1.1.running_mean\", \"module.conv1.1.running_var\", \"module.conv1.1.num_batches_tracked\", \"module.conv2.0.weight\", \"module.conv2.0.bias\", \"module.conv2.1.weight\", \"module.conv2.1.bias\", \"module.conv2.1.running_mean\", \"module.conv2.1.running_var\", \"module.conv2.1.num_batches_tracked\", \"module.conv3.0.weight\", \"module.conv3.0.bias\", \"module.conv3.1.weight\", \"module.conv3.1.bias\", \"module.conv3.1.running_mean\", \"module.conv3.1.running_var\", \"module.conv3.1.num_batches_tracked\", \"module.fc1.weight\", \"module.fc1.bias\", \"module.fc2.weight\", \"module.fc2.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Load saved model weights\u001b[39;00m\n\u001b[1;32m     84\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeightedGNNModels-5A/model_bin_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Update with correct path if needed\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m     88\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/shared/apps/anaconda3/2024.02/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CNN2D:\n\tMissing key(s) in state_dict: \"conv1.0.weight\", \"conv1.0.bias\", \"conv1.1.weight\", \"conv1.1.bias\", \"conv1.1.running_mean\", \"conv1.1.running_var\", \"conv2.0.weight\", \"conv2.0.bias\", \"conv2.1.weight\", \"conv2.1.bias\", \"conv2.1.running_mean\", \"conv2.1.running_var\", \"conv3.0.weight\", \"conv3.0.bias\", \"conv3.1.weight\", \"conv3.1.bias\", \"conv3.1.running_mean\", \"conv3.1.running_var\", \"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\". \n\tUnexpected key(s) in state_dict: \"module.conv1.0.weight\", \"module.conv1.0.bias\", \"module.conv1.1.weight\", \"module.conv1.1.bias\", \"module.conv1.1.running_mean\", \"module.conv1.1.running_var\", \"module.conv1.1.num_batches_tracked\", \"module.conv2.0.weight\", \"module.conv2.0.bias\", \"module.conv2.1.weight\", \"module.conv2.1.bias\", \"module.conv2.1.running_mean\", \"module.conv2.1.running_var\", \"module.conv2.1.num_batches_tracked\", \"module.conv3.0.weight\", \"module.conv3.0.bias\", \"module.conv3.1.weight\", \"module.conv3.1.bias\", \"module.conv3.1.running_mean\", \"module.conv3.1.running_var\", \"module.conv3.1.num_batches_tracked\", \"module.fc1.weight\", \"module.fc1.bias\", \"module.fc2.weight\", \"module.fc2.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the 2D CNN model in PyTorch\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 4 * 18, 128)  # Adjust based on input size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "models = []\n",
    "\n",
    "k = 1\n",
    "#version_number = 6\n",
    "\n",
    "for i in range (1, (k + 1)):\n",
    "    # Initialize model\n",
    "    model = CNN2D(input_channels=1).to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # Load saved model weights\n",
    "    model_path = f\"WeightedGNNModels-5A/model_bin_{i}.pth\"  # Update with correct path if needed\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_file(model, file_path, threshold=0.5):\n",
    "    # Load numpy array and convert it to a PyTorch tensor\n",
    "    grid = np.load(file_path)\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # Add batch and channel dims\n",
    "    grid_tensor = grid_tensor.to(device)\n",
    "    \n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(grid_tensor).squeeze(1)  # Shape: [1] after squeeze\n",
    "\n",
    "    # Apply sigmoid to get probabilities\n",
    "    prob = torch.sigmoid(output).item()\n",
    "\n",
    "    # Predict class based on threshold\n",
    "    predicted_class = int(prob >= threshold)\n",
    "\n",
    "    return predicted_class, prob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def evaluate_directory(model, directory):\n",
    "    files = glob.glob(f\"{directory}/*.npy\")  # Adjust path as needed\n",
    "    results = {}\n",
    "\n",
    "    false_negative_files = []\n",
    "    for file in files:\n",
    "        pred_class, probability = evaluate_file(model, file)\n",
    "        results[file] = {\"Predicted Class\": pred_class, \"Probability\": probability}\n",
    "        if pred_class == 0:\n",
    "            false_negative_files.append(file)\n",
    "    return results, false_negative_files\n",
    "\n",
    "directory_path = f\"../../../Data/SplitData/Cholesterol/cholesterol-graph-5A/Spies\"  # Update with your validation directory\n",
    "print(f\"{'ModelID':<10}{'#ofPos':<10}{'#ofNeg':<10}{'UnconfPos':<12}{'UnconfNeg':<12}{'SpyCaptureRate':<12}\")\n",
    "\n",
    "file_labels = {}\n",
    "file_raw_softmax = {}\n",
    "\n",
    "model_id = 1\n",
    "total_false_negative_files = []\n",
    "for model in models:\n",
    "    # Example usage:\n",
    "    evaluation_results, false_negative_files = evaluate_directory(model, directory_path)\n",
    "    total_false_negative_files.extend(false_negative_files)\n",
    "\n",
    "    # Print results\n",
    "    positives_count = 0\n",
    "    unconfident_positive_count = 0\n",
    "    false_negative_count = 0\n",
    "    unconfident_negative_count = 0\n",
    "    for file, result in evaluation_results.items():\n",
    "        if file not in file_labels:\n",
    "            file_labels[file] = 0            \n",
    "        file_labels[file] += result['Probability']\n",
    "        if result['Predicted Class'] == 1:\n",
    "            positives_count += 1\n",
    "        if result['Predicted Class'] == 0:\n",
    "            false_negative_count += 1\n",
    "    print(f\"{model_id:<10}{positives_count:<10}{false_negative_count:<10}{unconfident_positive_count:<12}{unconfident_negative_count:<12}{(positives_count / 154):<12}\")\n",
    "    model_id += 1\n",
    "\n",
    "overall_spy_capture_rate = 0\n",
    "\n",
    "with open(\"spy_capture_rates.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"average_score\"])  # Header row\n",
    "\n",
    "    print(f\"\\n{'FileName':<70}{'SpyCapture':<4}\")\n",
    "    for file, total_score in file_labels.items():\n",
    "        overall_spy_capture_rate += total_score/ k\n",
    "        writer.writerow([file, (total_score/ k)])\n",
    "        print(f\"{file:<70} {(total_score/ k):<4}\")\n",
    "\n",
    "print(\"Overall Spy Capture Rate:\", (overall_spy_capture_rate / 154))\n",
    "\n",
    "#find unlabeled with capture rate bigger than 0.97 treat as potential positive\n",
    "# if less than 0.06 potential negative, anything in between is unlabeled\n",
    "# if greater than 0.48 and less than 0.97 than likely positives but still unlabeled\n",
    "# if less than 0.48 and greater than 0.06 than likely negative but still unlabeled\n",
    "# take 6 colors (3 for positive and 3 for unlabeled) which are close to median (one median from each group) in 4 groups and do pca plot or tsne plot\n",
    "# for each category mark dataset that it's from\n",
    "# 2 pca's one for adjacency matrix and one for result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting results for propofol unlabeled\n",
    "file_labels = {}\n",
    "directory_path = f\"../../../Data/SplitData/Cholesterol/cholesterol-graph-5A/Test/Positive\"\n",
    "print(f\"{'ModelID':<10}{'#ofPos':<10}{'#ofNeg':<10}{'UnconfPos':<12}{'UnconfNeg':<12}{'PositiveRatio':<12}\")\n",
    "\n",
    "model_id = 1\n",
    "total_false_negative_files = []\n",
    "for model in models:\n",
    "    # Example usage:\n",
    "    evaluation_results, false_negative_files = evaluate_directory(model, directory_path)\n",
    "    total_false_negative_files.extend(false_negative_files)\n",
    "\n",
    "    # Print results\n",
    "    positives_count = 0\n",
    "    unconfident_positive_count = 0\n",
    "    false_negative_count = 0\n",
    "    unconfident_negative_count = 0\n",
    "    for file, result in evaluation_results.items():\n",
    "        if file not in file_labels:\n",
    "            file_labels[file] = 0            \n",
    "        file_labels[file] += result['Probability']\n",
    "        \n",
    "        #print(f\"File: {file}, Prediction: {result['Predicted Class']}, Confidence: {result['Confidence']:.4f}\")\n",
    "        if result['Predicted Class'] == 1:\n",
    "            positives_count += 1\n",
    "        if result['Predicted Class'] == 0:\n",
    "            false_negative_count += 1\n",
    "    print(f\"{model_id:<10}{positives_count:<10}{false_negative_count:<10}{unconfident_positive_count:<12}{unconfident_negative_count:<12}{(positives_count / 154):<12}\")\n",
    "    model_id += 1\n",
    "\n",
    "print(f\"\\n{'FileName':<100}{'Probabilities':<20}\")\n",
    "\n",
    "overall_spy_capture_rate = 0\n",
    "\n",
    "with open(\"test_positive_capture_rates.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"average_score\"])  # Header row\n",
    "\n",
    "    for file, total_score in file_labels.items():\n",
    "        average_score = total_score/ k\n",
    "        overall_spy_capture_rate += average_score\n",
    "        writer.writerow([file, average_score])\n",
    "\n",
    "        print(f\"{file:<100} {average_score:<20}\")\n",
    "\n",
    "print(\"Overall Positive Capture Rate:\", (overall_spy_capture_rate / 154))\n",
    "\n",
    "# make pca cluster of adjacency matrix for each dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting results for propofol unlabeled\n",
    "file_labels = {}\n",
    "directory_path = f\"../../../Data/SplitData/Cholesterol/cholesterol-graph-5A/Test/Unlabeled\"\n",
    "print(f\"{'ModelID':<10}{'#ofPos':<10}{'#ofNeg':<10}{'UnconfPos':<12}{'UnconfNeg':<12}{'PositiveRatio':<12}\")\n",
    "\n",
    "model_id = 1\n",
    "total_false_negative_files = []\n",
    "for model in models:\n",
    "    # Example usage:\n",
    "    evaluation_results, false_negative_files = evaluate_directory(model, directory_path)\n",
    "    total_false_negative_files.extend(false_negative_files)\n",
    "\n",
    "    # Print results\n",
    "    positives_count = 0\n",
    "    unconfident_positive_count = 0\n",
    "    false_negative_count = 0\n",
    "    unconfident_negative_count = 0\n",
    "    for file, result in evaluation_results.items():\n",
    "        if file not in file_labels:\n",
    "            file_labels[file] = 0            \n",
    "        file_labels[file] += result['Probability']\n",
    "        \n",
    "        #print(f\"File: {file}, Prediction: {result['Predicted Class']}, Confidence: {result['Confidence']:.4f}\")\n",
    "        if result['Predicted Class'] == 1:\n",
    "            positives_count += 1\n",
    "        if result['Predicted Class'] == 0:\n",
    "            false_negative_count += 1\n",
    "    print(f\"{model_id:<10}{positives_count:<10}{false_negative_count:<10}{unconfident_positive_count:<12}{unconfident_negative_count:<12}{(positives_count / 554):<12}\")\n",
    "    model_id += 1\n",
    "\n",
    "print(f\"\\n{'FileName':<100}{'Probabilities':<20}\")\n",
    "\n",
    "overall_spy_capture_rate = 0\n",
    "\n",
    "with open(\"test_unlabeled_capture_rates.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"average_score\"])  # Header row\n",
    "\n",
    "    for file, total_score in file_labels.items():\n",
    "        average_score = total_score/ k\n",
    "        overall_spy_capture_rate += average_score\n",
    "        writer.writerow([file, average_score])\n",
    "\n",
    "        print(f\"{file:<100} {average_score:<20}\")\n",
    "\n",
    "print(\"Overall Positive Capture Rate:\", (overall_spy_capture_rate / 554))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelID   #ofPos    #ofNeg    UnconfPos   UnconfNeg   PositiveRatio\n",
      "\n",
      "FileName                                                                                            Probabilities       \n",
      "Overall Positive Capture Rate: 0.0\n"
     ]
    }
   ],
   "source": [
    "# getting results for propofol unlabeled\n",
    "file_labels = {}\n",
    "directory_path = f\"../../../Data/SplitData/Cholesterol/cholesterol-graph-5A/Test/LikelyPositives\"\n",
    "print(f\"{'ModelID':<10}{'#ofPos':<10}{'#ofNeg':<10}{'UnconfPos':<12}{'UnconfNeg':<12}{'PositiveRatio':<12}\")\n",
    "\n",
    "model_id = 1\n",
    "total_false_negative_files = []\n",
    "for model in models:\n",
    "    # Example usage:\n",
    "    evaluation_results, false_negative_files = evaluate_directory(model, directory_path)\n",
    "    total_false_negative_files.extend(false_negative_files)\n",
    "\n",
    "    # Print results\n",
    "    positives_count = 0\n",
    "    unconfident_positive_count = 0\n",
    "    false_negative_count = 0\n",
    "    unconfident_negative_count = 0\n",
    "    for file, result in evaluation_results.items():\n",
    "        if file not in file_labels:\n",
    "            file_labels[file] = 0            \n",
    "        file_labels[file] += result['Probability']\n",
    "        \n",
    "        #print(f\"File: {file}, Prediction: {result['Predicted Class']}, Confidence: {result['Confidence']:.4f}\")\n",
    "        if result['Predicted Class'] == 1:\n",
    "            positives_count += 1\n",
    "        if result['Predicted Class'] == 0:\n",
    "            false_negative_count += 1\n",
    "    print(f\"{model_id:<10}{positives_count:<10}{false_negative_count:<10}{unconfident_positive_count:<12}{unconfident_negative_count:<12}{(positives_count / 369):<12}\")\n",
    "    model_id += 1\n",
    "\n",
    "print(f\"\\n{'FileName':<100}{'Probabilities':<20}\")\n",
    "\n",
    "overall_spy_capture_rate = 0\n",
    "\n",
    "with open(\"test_unlabeled_capture_rates.csv\", \"w\", newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"filename\", \"average_score\"])  # Header row\n",
    "\n",
    "    for file, total_score in file_labels.items():\n",
    "        average_score = total_score/ k\n",
    "        overall_spy_capture_rate += average_score\n",
    "        writer.writerow([file, average_score])\n",
    "\n",
    "        print(f\"{file:<100} {average_score:<20}\")\n",
    "\n",
    "print(\"Overall Positive Capture Rate:\", (overall_spy_capture_rate / 369))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
