{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a3808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool, GCNConv\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define GAT model for batched data\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(in_channels, out_channels, heads=1, concat=True, edge_dim=1)\n",
    "        self.pool = global_mean_pool  # Can also use global_max_pool or global_add_pool\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.norm = nn.BatchNorm1d(out_channels)\n",
    "        self.linear = torch.nn.Linear(out_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        out, attn_weights = self.gat(x, edge_index, edge_attr, return_attention_weights=True)\n",
    "        out = self.dropout(out)\n",
    "        out = self.pool(out, batch)  # Pool over nodes in each graph\n",
    "        out = self.norm(out)\n",
    "        out = self.dropout(out) \n",
    "        out = self.linear(out)\n",
    "        return out, attn_weights\n",
    "\n",
    "def gat_organize_graph_and_add_weight(file_path, label):\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    inverse_distance = data['inverse_distance']\n",
    "    encoded_matrix = data['encoded_matrix']\n",
    "\n",
    "    x = torch.tensor(encoded_matrix, dtype=torch.float32)\n",
    "    adj = torch.tensor(inverse_distance, dtype=torch.float32)\n",
    "\n",
    "    # Normalize adjacency (row-normalize)\n",
    "    adj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    # Create edge_index and edge weights\n",
    "    edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "    edge_weight = adj[adj > 0]\n",
    "\n",
    "    y = torch.tensor([label], dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        \n",
    "        self.conv2 = GCNConv(32, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.conv3 = GCNConv(64, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.dropout_gcn = nn.Dropout(0.2)\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_weight, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_gcn(x)\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_gcn(x)\n",
    "\n",
    "        x = self.conv3(x, edge_index, edge_weight)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout_gcn(x)\n",
    "\n",
    "        # Global pooling to get graph-level representation\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def gcn_organize_graph_and_add_weight(file_path, label):\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    inverse_distance = data['inverse_distance']\n",
    "    encoded_matrix = data['encoded_matrix']\n",
    "\n",
    "    x = torch.tensor(encoded_matrix, dtype=torch.float32)\n",
    "    adj = torch.tensor(inverse_distance, dtype=torch.float32)\n",
    "\n",
    "    # Normalize adjacency (row-normalize)\n",
    "    #adj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    # Create edge_index and edge weights\n",
    "    edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "    edge_weight = adj[adj > 0]\n",
    "\n",
    "    y = torch.tensor([label], dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "\n",
    "# Define the 2D CNN model in PyTorch\n",
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CNN2D, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(128 * 25 * 8, 128)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57fccafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Paths (your dirs)\n",
    "# -----------------------------\n",
    "gat_gcn_spies = \"../CLR_Ligand_Data/cholesterol-separate-graphs-clr_exp4/Spies\"\n",
    "gnn_spies     = \"../CLR_Ligand_Data/cholesterol-graph-clr_exp1/Spies\"\n",
    "\n",
    "gat_gcn_positive = \"../CLR_Ligand_Data/cholesterol-separate-graphs-clr_exp4/Test/Positive\"\n",
    "gnn_positive     = \"cholesterol-ivan-clr/positive\"\n",
    "\n",
    "gat_gcn_unlabeled = \"cholesterol-separate-sep-clr/unlabeled\"\n",
    "gnn_unlabeled     = \"cholesterol-sep-clr/unlabeled\"\n",
    "\n",
    "OUT_ROOT = \"./PU_EvalOutputs_Ensemble\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f01139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# File helpers\n",
    "# -----------------------------\n",
    "def list_npy_files(dir_path: str):\n",
    "    return sorted(glob.glob(os.path.join(dir_path, \"*.npy\")))\n",
    "\n",
    "def load_npy_dict(file_path: str):\n",
    "    return np.load(file_path, allow_pickle=True).item()\n",
    "\n",
    "def load_cnn_matrix(file_path: str) -> np.ndarray:\n",
    "    x = np.load(file_path, allow_pickle=True)\n",
    "    # Must be a numeric 2D matrix\n",
    "    if not isinstance(x, np.ndarray):\n",
    "        raise ValueError(f\"CNN file did not load as ndarray: {file_path}, type={type(x)}\")\n",
    "    if x.ndim != 2:\n",
    "        raise ValueError(f\"CNN matrix must be 2D: {file_path}, got shape {x.shape}\")\n",
    "    if x.dtype == object:\n",
    "        raise ValueError(f\"CNN matrix should not be dtype=object: {file_path}, got dtype=object with shape {x.shape}\")\n",
    "    return x\n",
    "\n",
    "def infer_graph_input_dim(dir_path: str) -> int:\n",
    "    files = list_npy_files(dir_path)\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No .npy files found in: {dir_path}\")\n",
    "    d = load_npy_dict(files[0])\n",
    "    return int(d[\"encoded_matrix\"].shape[1])\n",
    "\n",
    "def organize_graph(file_path: str, normalize_adj: bool) -> Data:\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    adj = torch.tensor(data['inverse_distance'], dtype=torch.float32)\n",
    "    x = torch.tensor(data['encoded_matrix'], dtype=torch.float32)\n",
    "\n",
    "    if normalize_adj:\n",
    "        adj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "    edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "    edge_weight = adj[adj > 0]\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight)\n",
    "\n",
    "def ensure_dir(p: str):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Labeling method (from spies percentiles)\n",
    "# -----------------------------\n",
    "def compute_spy_percentiles(spy_probs: np.ndarray):\n",
    "    p25, p50, p75 = np.percentile(spy_probs, [25, 50, 75])\n",
    "    return {\n",
    "        \"p25\": float(p25),\n",
    "        \"p50\": float(p50),\n",
    "        \"p75\": float(p75),\n",
    "        \"n_spies\": int(len(spy_probs)),\n",
    "        \"min\": float(np.min(spy_probs)) if len(spy_probs) else None,\n",
    "        \"max\": float(np.max(spy_probs)) if len(spy_probs) else None,\n",
    "        \"mean\": float(np.mean(spy_probs)) if len(spy_probs) else None,\n",
    "        \"std\": float(np.std(spy_probs)) if len(spy_probs) else None,\n",
    "    }\n",
    "\n",
    "def label_from_percentiles(prob: float, stats: dict) -> str:\n",
    "    p25, p50, p75 = stats[\"p25\"], stats[\"p50\"], stats[\"p75\"]\n",
    "    if prob <= p25:\n",
    "        return \"Negative\"\n",
    "    elif prob <= p50:\n",
    "        return \"PseudoNegative\"\n",
    "    elif prob <= p75:\n",
    "        return \"PseudoPositive\"\n",
    "    else:\n",
    "        return \"Positive\"\n",
    "\n",
    "def plot_label_histogram(df: pd.DataFrame, title: str, out_png: str):\n",
    "    order = [\"Negative\", \"PseudoNegative\", \"PseudoPositive\", \"Positive\"]\n",
    "    counts = df[\"label\"].value_counts().reindex(order, fill_value=0)\n",
    "    plt.figure()\n",
    "    plt.bar(counts.index.tolist(), counts.values.tolist())\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Label\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=20, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_png, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def resolve_ensemble_checkpoints(pattern_or_dir: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Give either:\n",
    "      - a glob pattern:  \"/path/to/gat_submodels/*.pt\"\n",
    "      - or a directory: \"/path/to/gat_submodels\"\n",
    "    \"\"\"\n",
    "    if os.path.isdir(pattern_or_dir):\n",
    "        ckpts = sorted(glob.glob(os.path.join(pattern_or_dir, \"*.pt\"))) + \\\n",
    "                sorted(glob.glob(os.path.join(pattern_or_dir, \"*.pth\")))\n",
    "    else:\n",
    "        ckpts = sorted(glob.glob(pattern_or_dir))\n",
    "    if len(ckpts) == 0:\n",
    "        raise FileNotFoundError(f\"No checkpoints found for: {pattern_or_dir}\")\n",
    "    return ckpts\n",
    "\n",
    "def load_state_dict_into(model: nn.Module, ckpt_path: str):\n",
    "    sd = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "    # supports either raw state_dict or a dict with 'model_state_dict'\n",
    "    if isinstance(sd, dict) and \"model_state_dict\" in sd:\n",
    "        sd = sd[\"model_state_dict\"]\n",
    "    model.load_state_dict(sd)\n",
    "    return model\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_predict_probs_graph(\n",
    "    model_ctor,\n",
    "    gat_bool,\n",
    "    ckpt_paths: list[str],\n",
    "    files: list[str],\n",
    "    batch_size: int = 16,\n",
    "    num_workers: int = 0,\n",
    ") -> list[dict]:\n",
    "\n",
    "    if len(files) == 0:\n",
    "        return []\n",
    "\n",
    "    data_list = [organize_graph(fp, gat_bool) for fp in files]\n",
    "    loader = DataLoader(data_list, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    prob_sum = np.zeros(len(files), dtype=np.float64)\n",
    "\n",
    "    for k, ckpt in enumerate(ckpt_paths):\n",
    "        model = model_ctor().to(device)\n",
    "        model = load_state_dict_into(model, ckpt)\n",
    "        model.eval()\n",
    "\n",
    "        idx = 0\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "\n",
    "            if isinstance(model, GAT):\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            else:\n",
    "                out = model(batch)\n",
    "\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            logits = logits.view(-1)\n",
    "\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "\n",
    "            bsz = len(probs)\n",
    "            prob_sum[idx:idx+bsz] += probs\n",
    "            idx += bsz\n",
    "\n",
    "        print(f\"  submodel {k+1}/{len(ckpt_paths)} done: {os.path.basename(ckpt)}\")\n",
    "\n",
    "    prob_avg = prob_sum / float(len(ckpt_paths))\n",
    "\n",
    "    return [\n",
    "        {\"file\": os.path.basename(fp), \"path\": fp, \"prob\": float(prob_avg[i])}\n",
    "        for i, fp in enumerate(files)\n",
    "    ]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Ensemble inference (CNN2D)\n",
    "# -----------------------------\n",
    "def encoded_matrix_to_cnn_input(encoded_matrix: np.ndarray, cnn_h: int, cnn_w: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    MUST match how you trained CNN2D.\n",
    "\n",
    "    Here we assume 1-channel images of shape (cnn_h, cnn_w).\n",
    "    We reshape only if total size matches.\n",
    "    \"\"\"\n",
    "    if encoded_matrix.ndim != 2:\n",
    "        raise ValueError(f\"encoded_matrix must be 2D, got shape {encoded_matrix.shape}\")\n",
    "\n",
    "    img = encoded_matrix\n",
    "    if img.shape != (cnn_h, cnn_w):\n",
    "        if img.size != cnn_h * cnn_w:\n",
    "            raise ValueError(\n",
    "                f\"encoded_matrix shape {img.shape} (size={img.size}) cannot reshape to ({cnn_h},{cnn_w}). \"\n",
    "                f\"Update cnn_h/cnn_w to your training shape.\"\n",
    "            )\n",
    "        img = img.reshape(cnn_h, cnn_w)\n",
    "\n",
    "    return torch.tensor(img, dtype=torch.float32).unsqueeze(0)  # [C=1,H,W]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def ensemble_predict_probs_cnn2d(\n",
    "    ckpt_paths: list[str],\n",
    "    files: list[str],\n",
    "    batch_size: int = 32,\n",
    "    cnn_h: int = 200,\n",
    "    cnn_w: int = 65,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Averages PROBABILITIES across CNN2D submodels.\n",
    "    \"\"\"\n",
    "    if len(files) == 0:\n",
    "        return []\n",
    "\n",
    "    prob_sum = np.zeros(len(files), dtype=np.float64)\n",
    "\n",
    "    for k, ckpt in enumerate(ckpt_paths):\n",
    "        model = CNN2D(input_channels=1).to(device)\n",
    "        model = load_state_dict_into(model, ckpt)\n",
    "        model.eval()\n",
    "\n",
    "        for i in range(0, len(files), batch_size):\n",
    "            batch_files = files[i:i+batch_size]\n",
    "            xs = []\n",
    "            for fp in batch_files:\n",
    "                enc = load_cnn_matrix(fp)\n",
    "                xs.append(encoded_matrix_to_cnn_input(enc, cnn_h=cnn_h, cnn_w=cnn_w))\n",
    "            x_batch = torch.stack(xs, dim=0).to(device)  # [B,1,H,W]\n",
    "\n",
    "            logits = model(x_batch).view(-1)\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            prob_sum[i:i+len(probs)] += probs\n",
    "\n",
    "        print(f\"  submodel {k+1}/{len(ckpt_paths)} done: {os.path.basename(ckpt)}\")\n",
    "\n",
    "    prob_avg = prob_sum / float(len(ckpt_paths))\n",
    "    rows = []\n",
    "    for i, fp in enumerate(files):\n",
    "        rows.append({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"path\": fp,\n",
    "            \"prob\": float(prob_avg[i]),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Run PU eval for a given ensemble\n",
    "# -----------------------------\n",
    "def run_pu_eval_ensemble(\n",
    "    model_name: str,\n",
    "    out_dir: str,\n",
    "    spies_dir: str,\n",
    "    eval_dir: str,\n",
    "    predict_rows_fn,     # function(files)->rows\n",
    "):\n",
    "    ensure_dir(out_dir)\n",
    "\n",
    "    spy_files = list_npy_files(spies_dir)\n",
    "    eval_files = list_npy_files(eval_dir)\n",
    "\n",
    "    if len(spy_files) == 0:\n",
    "        raise FileNotFoundError(f\"[{model_name}] no spies in {spies_dir}\")\n",
    "    if len(eval_files) == 0:\n",
    "        raise FileNotFoundError(f\"[{model_name}] no eval files in {eval_dir}\")\n",
    "\n",
    "    print(f\"\\n[{model_name}] Predicting spies ({len(spy_files)}) ...\")\n",
    "    spy_rows = predict_rows_fn(spy_files)\n",
    "    spy_probs = np.array([r[\"prob\"] for r in spy_rows], dtype=float)\n",
    "\n",
    "    spy_stats = compute_spy_percentiles(spy_probs)\n",
    "    spy_stats[\"model\"] = model_name\n",
    "    spy_stats[\"spies_dir\"] = spies_dir\n",
    "\n",
    "    # save spies stats + spies probs\n",
    "    with open(os.path.join(out_dir, f\"{model_name}_spies_stats.json\"), \"w\") as f:\n",
    "        json.dump(spy_stats, f, indent=2)\n",
    "    pd.DataFrame([spy_stats]).to_csv(os.path.join(out_dir, f\"{model_name}_spies_stats.csv\"), index=False)\n",
    "    pd.DataFrame(spy_rows).to_csv(os.path.join(out_dir, f\"{model_name}_spies_probs.csv\"), index=False)\n",
    "\n",
    "    print(f\"[{model_name}] spies thresholds: p25={spy_stats['p25']:.4f}, p50={spy_stats['p50']:.4f}, p75={spy_stats['p75']:.4f}\")\n",
    "\n",
    "    print(f\"\\n[{model_name}] Predicting eval dir ({len(eval_files)}) ...\")\n",
    "    eval_rows = predict_rows_fn(eval_files)\n",
    "\n",
    "    # label using spies thresholds\n",
    "    for r in eval_rows:\n",
    "        r[\"label\"] = label_from_percentiles(r[\"prob\"], spy_stats)\n",
    "\n",
    "    df_eval = pd.DataFrame(eval_rows)\n",
    "    out_csv = os.path.join(out_dir, f\"{model_name}_eval_probs_and_labels.csv\")\n",
    "    df_eval.to_csv(out_csv, index=False)\n",
    "\n",
    "    out_png = os.path.join(out_dir, f\"{model_name}_eval_label_hist.png\")\n",
    "    plot_label_histogram(df_eval, f\"{model_name} label counts (ensemble avg; thresholds from spies)\", out_png)\n",
    "\n",
    "    print(f\"\\n[{model_name}] label counts:\\n{df_eval['label'].value_counts()}\")\n",
    "    print(f\"Saved: {out_csv}\")\n",
    "    print(f\"Saved: {out_png}\")\n",
    "    print(f\"Saved: {os.path.join(out_dir, f'{model_name}_spies_stats.json')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "481a7d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT submodels: 50\n",
      "GCN submodels: 50\n",
      "CNN2D submodels: 50\n",
      "\n",
      "[GAT_ENSEMBLE] Predicting spies (154) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "[GAT_ENSEMBLE] spies thresholds: p25=0.7348, p50=0.8493, p75=0.9093\n",
      "\n",
      "[GAT_ENSEMBLE] Predicting eval dir (61) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "\n",
      "[GAT_ENSEMBLE] label counts:\n",
      "label\n",
      "Negative          55\n",
      "PseudoNegative     6\n",
      "Name: count, dtype: int64\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GAT_separate_graphs_sep/GAT_ENSEMBLE_eval_probs_and_labels.csv\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GAT_separate_graphs_sep/GAT_ENSEMBLE_eval_label_hist.png\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GAT_separate_graphs_sep/GAT_ENSEMBLE_spies_stats.json\n",
      "\n",
      "\n",
      "[GCN_ENSEMBLE] Predicting spies (154) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "[GCN_ENSEMBLE] spies thresholds: p25=0.7033, p50=0.8212, p75=0.8803\n",
      "\n",
      "[GCN_ENSEMBLE] Predicting eval dir (61) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "\n",
      "[GCN_ENSEMBLE] label counts:\n",
      "label\n",
      "Negative          29\n",
      "Positive          14\n",
      "PseudoNegative    11\n",
      "PseudoPositive     7\n",
      "Name: count, dtype: int64\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GCN_separate_graphs_sep/GCN_ENSEMBLE_eval_probs_and_labels.csv\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GCN_separate_graphs_sep/GCN_ENSEMBLE_eval_label_hist.png\n",
      "Saved: ./PU_EvalOutputs_Ensemble/GCN_separate_graphs_sep/GCN_ENSEMBLE_spies_stats.json\n",
      "\n",
      "\n",
      "[CNN2D_ENSEMBLE] Predicting spies (154) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "[CNN2D_ENSEMBLE] spies thresholds: p25=0.7723, p50=0.8170, p75=0.8694\n",
      "\n",
      "[CNN2D_ENSEMBLE] Predicting eval dir (61) ...\n",
      "  submodel 1/50 done: model_bin_1.pth\n",
      "  submodel 2/50 done: model_bin_10.pth\n",
      "  submodel 3/50 done: model_bin_11.pth\n",
      "  submodel 4/50 done: model_bin_12.pth\n",
      "  submodel 5/50 done: model_bin_13.pth\n",
      "  submodel 6/50 done: model_bin_14.pth\n",
      "  submodel 7/50 done: model_bin_15.pth\n",
      "  submodel 8/50 done: model_bin_16.pth\n",
      "  submodel 9/50 done: model_bin_17.pth\n",
      "  submodel 10/50 done: model_bin_18.pth\n",
      "  submodel 11/50 done: model_bin_19.pth\n",
      "  submodel 12/50 done: model_bin_2.pth\n",
      "  submodel 13/50 done: model_bin_20.pth\n",
      "  submodel 14/50 done: model_bin_21.pth\n",
      "  submodel 15/50 done: model_bin_22.pth\n",
      "  submodel 16/50 done: model_bin_23.pth\n",
      "  submodel 17/50 done: model_bin_24.pth\n",
      "  submodel 18/50 done: model_bin_25.pth\n",
      "  submodel 19/50 done: model_bin_26.pth\n",
      "  submodel 20/50 done: model_bin_27.pth\n",
      "  submodel 21/50 done: model_bin_28.pth\n",
      "  submodel 22/50 done: model_bin_29.pth\n",
      "  submodel 23/50 done: model_bin_3.pth\n",
      "  submodel 24/50 done: model_bin_30.pth\n",
      "  submodel 25/50 done: model_bin_31.pth\n",
      "  submodel 26/50 done: model_bin_32.pth\n",
      "  submodel 27/50 done: model_bin_33.pth\n",
      "  submodel 28/50 done: model_bin_34.pth\n",
      "  submodel 29/50 done: model_bin_35.pth\n",
      "  submodel 30/50 done: model_bin_36.pth\n",
      "  submodel 31/50 done: model_bin_37.pth\n",
      "  submodel 32/50 done: model_bin_38.pth\n",
      "  submodel 33/50 done: model_bin_39.pth\n",
      "  submodel 34/50 done: model_bin_4.pth\n",
      "  submodel 35/50 done: model_bin_40.pth\n",
      "  submodel 36/50 done: model_bin_41.pth\n",
      "  submodel 37/50 done: model_bin_42.pth\n",
      "  submodel 38/50 done: model_bin_43.pth\n",
      "  submodel 39/50 done: model_bin_44.pth\n",
      "  submodel 40/50 done: model_bin_45.pth\n",
      "  submodel 41/50 done: model_bin_46.pth\n",
      "  submodel 42/50 done: model_bin_47.pth\n",
      "  submodel 43/50 done: model_bin_48.pth\n",
      "  submodel 44/50 done: model_bin_49.pth\n",
      "  submodel 45/50 done: model_bin_5.pth\n",
      "  submodel 46/50 done: model_bin_50.pth\n",
      "  submodel 47/50 done: model_bin_6.pth\n",
      "  submodel 48/50 done: model_bin_7.pth\n",
      "  submodel 49/50 done: model_bin_8.pth\n",
      "  submodel 50/50 done: model_bin_9.pth\n",
      "\n",
      "[CNN2D_ENSEMBLE] label counts:\n",
      "label\n",
      "Negative          38\n",
      "PseudoPositive    12\n",
      "Positive           6\n",
      "PseudoNegative     5\n",
      "Name: count, dtype: int64\n",
      "Saved: ./PU_EvalOutputs_Ensemble/CNN2D_graph_format_sep/CNN2D_ENSEMBLE_eval_probs_and_labels.csv\n",
      "Saved: ./PU_EvalOutputs_Ensemble/CNN2D_graph_format_sep/CNN2D_ENSEMBLE_eval_label_hist.png\n",
      "Saved: ./PU_EvalOutputs_Ensemble/CNN2D_graph_format_sep/CNN2D_ENSEMBLE_spies_stats.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GAT_ENSEMBLE = \"../CLR_Ligand_Training/GAT-CLR_Exp4/Models/*.pth\"\n",
    "GCN_ENSEMBLE = \"../CLR_Ligand_Training/GCN-CLR_Exp4/*.pth\"\n",
    "CNN_ENSEMBLE = \"../CLR_Ligand_Training/GNN-CLR_Exp1/*.pth\"\n",
    "\n",
    "gat_ckpts = resolve_ensemble_checkpoints(GAT_ENSEMBLE)\n",
    "gcn_ckpts = resolve_ensemble_checkpoints(GCN_ENSEMBLE)\n",
    "cnn_ckpts = resolve_ensemble_checkpoints(CNN_ENSEMBLE)\n",
    "\n",
    "print(\"GAT submodels:\", len(gat_ckpts))\n",
    "print(\"GCN submodels:\", len(gcn_ckpts))\n",
    "print(\"CNN2D submodels:\", len(cnn_ckpts))\n",
    "\n",
    "GRAPH_INPUT_DIM = infer_graph_input_dim(gat_gcn_spies)\n",
    "\n",
    "def make_gat():\n",
    "    return GAT(in_channels=GRAPH_INPUT_DIM, out_channels=32, dropout_p=0.1)\n",
    "\n",
    "def make_gcn():\n",
    "    return GCN(input_dim=GRAPH_INPUT_DIM)\n",
    "\n",
    "run_pu_eval_ensemble(\n",
    "    model_name=\"GAT_ENSEMBLE\",\n",
    "    out_dir=os.path.join(OUT_ROOT, \"GAT_separate_graphs_sep\"),\n",
    "    spies_dir=gat_gcn_spies,\n",
    "    eval_dir=gat_gcn_unlabeled,\n",
    "    predict_rows_fn=lambda files: ensemble_predict_probs_graph(\n",
    "        model_ctor=make_gat,\n",
    "        gat_bool=True,\n",
    "        ckpt_paths=gat_ckpts,\n",
    "        files=files,\n",
    "        batch_size=16\n",
    "    ),\n",
    ")\n",
    "\n",
    "run_pu_eval_ensemble(\n",
    "    model_name=\"GCN_ENSEMBLE\",\n",
    "    out_dir=os.path.join(OUT_ROOT, \"GCN_separate_graphs_sep\"),\n",
    "    spies_dir=gat_gcn_spies,\n",
    "    eval_dir=gat_gcn_unlabeled,\n",
    "    predict_rows_fn=lambda files: ensemble_predict_probs_graph(\n",
    "        model_ctor=make_gcn,\n",
    "        gat_bool=False,\n",
    "        ckpt_paths=gcn_ckpts,\n",
    "        files=files,\n",
    "        batch_size=16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "run_pu_eval_ensemble(\n",
    "    model_name=\"CNN2D_ENSEMBLE\",\n",
    "    out_dir=os.path.join(OUT_ROOT, \"CNN2D_graph_format_sep\"),\n",
    "    spies_dir=gnn_spies,\n",
    "    eval_dir=gnn_unlabeled,\n",
    "    predict_rows_fn=lambda files: ensemble_predict_probs_cnn2d(\n",
    "        ckpt_paths=cnn_ckpts,\n",
    "        files=files,\n",
    "        batch_size=32,\n",
    "        cnn_h=200,\n",
    "        cnn_w=65,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f6c69e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'PU_EvalOutputs_Ensemble/CNN2D_graph_format_1_16/CNN2D_ENSEMBLE_eval_probs_and_labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     57\u001b[0m model_dfs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, csv_path \u001b[38;5;129;01min\u001b[39;00m MODEL_FILES\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 59\u001b[0m     mdf \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_eval_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m     mdf \u001b[38;5;241m=\u001b[39m mdf\u001b[38;5;241m.\u001b[39mmerge(occ, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid4\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# only keep IDs that have occupancy info\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     model_dfs[model_name] \u001b[38;5;241m=\u001b[39m mdf\n",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m, in \u001b[0;36mload_model_eval_csv\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model_eval_csv\u001b[39m(path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m---> 33\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid4\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(extract_id_from_eval_filecol)\n\u001b[1;32m     35\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid4\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'PU_EvalOutputs_Ensemble/CNN2D_graph_format_1_16/CNN2D_ENSEMBLE_eval_probs_and_labels.csv'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# =========================\n",
    "# Paths\n",
    "# =========================\n",
    "BASE = Path(\".\")  # or Path(\".\") if you're already inside it\n",
    "\n",
    "MODEL_FILES = {\n",
    "    \"GNN\": BASE / \"PU_EvalOutputs_Ensemble/CNN2D_graph_format_1_16/CNN2D_ENSEMBLE_eval_probs_and_labels.csv\",\n",
    "    \"GAT\": BASE / \"PU_EvalOutputs_Ensemble/GAT_separate_graphs_1_16/GAT_ENSEMBLE_eval_probs_and_labels.csv\",\n",
    "    \"GCN\": BASE / \"PU_EvalOutputs_Ensemble/GCN_separate_graphs_1_16/GCN_ENSEMBLE_eval_probs_and_labels.csv\",\n",
    "}\n",
    "\n",
    "OCC_FILE = BASE / \"average_delet_two_vaue_3dcnn_data.csv\"\n",
    "OCC_PREFIX = \"1_16-piezo-graph-5A\"  # only use rows whose filename starts with this\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def extract_id_from_eval_filecol(s: str) -> str | None:\n",
    "    m = re.match(r\"(\\d{4})\", str(s))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def extract_id_from_occ_filename(s: str) -> str | None:\n",
    "    m = re.search(r\"CHL1_(\\d{4})\", str(s))\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def load_model_eval_csv(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"id4\"] = df[\"file\"].apply(extract_id_from_eval_filecol)\n",
    "    df = df.dropna(subset=[\"id4\"]).copy()\n",
    "    df[\"prob\"] = pd.to_numeric(df[\"prob\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"prob\"]).copy()\n",
    "    return df\n",
    "\n",
    "def load_occupancy_csv(path: Path, prefix: str) -> pd.DataFrame:\n",
    "    occ = pd.read_csv(path)\n",
    "    occ = occ[occ[\"filename\"].astype(str).str.startswith(prefix)].copy()\n",
    "    occ[\"id4\"] = occ[\"filename\"].apply(extract_id_from_occ_filename)\n",
    "    occ = occ.dropna(subset=[\"id4\"]).copy()\n",
    "    occ[\"high_occupancy\"] = pd.to_numeric(occ[\"high_occupancy\"], errors=\"coerce\")\n",
    "    occ = occ.dropna(subset=[\"high_occupancy\"]).copy()\n",
    "\n",
    "    # If duplicates exist per id4, keep first (change to mean/max if desired)\n",
    "    occ = occ.sort_values(\"id4\").drop_duplicates(\"id4\", keep=\"first\")\n",
    "    return occ[[\"id4\", \"high_occupancy\"]]\n",
    "\n",
    "# =========================\n",
    "# Load + merge\n",
    "# =========================\n",
    "occ = load_occupancy_csv(OCC_FILE, OCC_PREFIX)\n",
    "\n",
    "model_dfs = {}\n",
    "for model_name, csv_path in MODEL_FILES.items():\n",
    "    mdf = load_model_eval_csv(csv_path)\n",
    "    mdf = mdf.merge(occ, on=\"id4\", how=\"inner\")  # only keep IDs that have occupancy info\n",
    "    model_dfs[model_name] = mdf\n",
    "\n",
    "# =========================\n",
    "# Fixed bins: 0-20, 20-40, 40-60, 60-80, 80-100\n",
    "# =========================\n",
    "edges = np.array([0, 20, 40, 60, 80, 100], dtype=float)\n",
    "bin_labels = [\"020\", \"2040\", \"4060\", \"6080\", \"80100\"]\n",
    "\n",
    "# right=False makes bins [0,20), [20,40), ... [80,100)\n",
    "# We'll include 100 explicitly by clipping >100 to 100 (just in case)\n",
    "for k, df in model_dfs.items():\n",
    "    df = df.copy()\n",
    "    df[\"high_occupancy\"] = df[\"high_occupancy\"].clip(lower=0, upper=100)\n",
    "\n",
    "    df[\"occ_bin\"] = pd.cut(\n",
    "        df[\"high_occupancy\"],\n",
    "        bins=edges,\n",
    "        include_lowest=True,\n",
    "        right=False,            # left-closed, right-open\n",
    "        labels=bin_labels\n",
    "    )\n",
    "\n",
    "    # Put exact 100 into the last bin (because right=False would exclude 100)\n",
    "    df.loc[df[\"high_occupancy\"] == 100, \"occ_bin\"] = \"80100\"\n",
    "\n",
    "    model_dfs[k] = df.dropna(subset=[\"occ_bin\"]).copy()\n",
    "\n",
    "# =========================\n",
    "# Plot: side-by-side\n",
    "# - Boxplots: probability (left y-axis)\n",
    "# - Bars: # samples (right y-axis)\n",
    "# =========================\n",
    "models_in_order = [\"GNN\", \"GAT\", \"GCN\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(models_in_order), figsize=(18, 6), sharey=True)\n",
    "if len(models_in_order) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "x_positions = np.arange(len(bin_labels))\n",
    "\n",
    "for ax, model_name in zip(axes, models_in_order):\n",
    "    df = model_dfs[model_name].copy()\n",
    "    df[\"occ_bin\"] = df[\"occ_bin\"].astype(\n",
    "        pd.CategoricalDtype(categories=bin_labels, ordered=True)\n",
    "    )\n",
    "\n",
    "    # Per-bin probability arrays\n",
    "    data_per_bin = [df.loc[df[\"occ_bin\"] == b, \"prob\"].values for b in bin_labels]\n",
    "    counts = [len(arr) for arr in data_per_bin]\n",
    "\n",
    "    # Mean probability per bin (ignore empty bins safely)\n",
    "    means = [\n",
    "        np.mean(arr) if len(arr) > 0 else np.nan\n",
    "        for arr in data_per_bin\n",
    "    ]\n",
    "\n",
    "    # --- Boxplot (probability distribution)\n",
    "    ax.boxplot(\n",
    "        data_per_bin,\n",
    "        positions=x_positions,\n",
    "        widths=0.6,\n",
    "        patch_artist=False,\n",
    "        showfliers=True\n",
    "    )\n",
    "\n",
    "    # --- Mean overlay\n",
    "    ax.plot(\n",
    "        x_positions,\n",
    "        means,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"-\",\n",
    "        linewidth=2,\n",
    "        label=\"Mean probability\"\n",
    "    )\n",
    "\n",
    "    ax.set_title(model_name)\n",
    "    ax.set_xlabel(\"Residue occupancy (binned)\")\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(bin_labels, rotation=0)\n",
    "\n",
    "    if ax is axes[0]:\n",
    "        ax.set_ylabel(\"Probability score\")\n",
    "\n",
    "    # --- Sample count bars (secondary axis)\n",
    "    ax2 = ax.twinx()\n",
    "    ax2.bar(x_positions, counts, alpha=0.25, width=0.8)\n",
    "    ax2.set_ylabel(\"# samples\")\n",
    "    ax2.set_ylim(0, max(counts) * 1.15 if max(counts) > 0 else 1)\n",
    "\n",
    "    # Legend (only once to avoid clutter)\n",
    "    ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
