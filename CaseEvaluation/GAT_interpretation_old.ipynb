{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d7d005d-428b-4c94-81c0-1f33a90af1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GATConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "from matplotlib.collections import PatchCollection\n",
    "import csv\n",
    "\n",
    "def organize_graph_and_add_weight(file_path, label):\n",
    "    \"\"\"Your graph preprocessor — same as you have.\"\"\"\n",
    "    data = np.load(file_path, allow_pickle=True).item()\n",
    "    inverse_distance = data['inverse_distance']\n",
    "    encoded_matrix = data['encoded_matrix']\n",
    "\n",
    "    x = torch.tensor(encoded_matrix, dtype=torch.float32)\n",
    "    adj = torch.tensor(inverse_distance, dtype=torch.float32)\n",
    "\n",
    "    adj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "    edge_weight = adj[adj > 0]\n",
    "\n",
    "    y = torch.tensor([label], dtype=torch.float32)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.gat = GATConv(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            heads=1, \n",
    "            concat=True, \n",
    "            edge_dim=1\n",
    "        )\n",
    "        self.norm = nn.BatchNorm1d(out_channels)  #\n",
    "        self.pool = global_mean_pool\n",
    "        self.dropout = nn.Dropout(p=dropout_p)\n",
    "        self.linear = nn.Linear(out_channels, 1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch):\n",
    "        out, attn_weights = self.gat(\n",
    "            x, edge_index, edge_attr, return_attention_weights=True\n",
    "        )\n",
    "        out = self.norm(out)  # \n",
    "        out = self.dropout(out)\n",
    "        out = self.pool(out, batch)\n",
    "        out = self.dropout(out)\n",
    "        out = self.linear(out)\n",
    "        out = self.activation(out)\n",
    "        return out, attn_weights\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def get_important_edges_from_npy(\n",
    "    npy_file,\n",
    "    model, \n",
    "    device\n",
    "):\n",
    "    \"\"\"\n",
    "    Load a graph from .npy, run GAT, compute Attention × Gradient importance.\n",
    "    Return both the DataFrame and the encoded_matrix!\n",
    "    \"\"\"\n",
    "    # === Load graph ===\n",
    "    data = np.load(npy_file, allow_pickle=True).item()\n",
    "    inverse_distance = data['inverse_distance']\n",
    "    encoded_matrix = data['encoded_matrix']\n",
    "\n",
    "    # Build PyG Data\n",
    "    x = torch.tensor(encoded_matrix, dtype=torch.float32)\n",
    "    adj = torch.tensor(inverse_distance, dtype=torch.float32)\n",
    "    adj = adj / (adj.sum(dim=1, keepdim=True) + 1e-8)\n",
    "    edge_index = (adj > 0).nonzero(as_tuple=False).t()\n",
    "    edge_weight = adj[adj > 0] \n",
    "    y = torch.tensor([1.0], dtype=torch.float32)\n",
    "\n",
    "    from torch_geometric.data import Data\n",
    "    graph = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)\n",
    "    batch = torch.zeros(graph.x.size(0), dtype=torch.long)\n",
    "\n",
    "    graph = graph.to(device)\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    # === Forward pass ===\n",
    "    output, (edge_index, alpha) = model(\n",
    "        graph.x, graph.edge_index, graph.edge_attr, batch\n",
    "    )\n",
    "    alpha.retain_grad()\n",
    "\n",
    "    # === Backward ===\n",
    "    model.zero_grad()\n",
    "    output[0].backward()\n",
    "\n",
    "    alpha_importance = alpha* alpha.grad\n",
    "\n",
    "    edge_src = edge_index[0].cpu().numpy().reshape(-1)\n",
    "    edge_dst = edge_index[1].cpu().numpy().reshape(-1)\n",
    "    attention = alpha.detach().cpu().numpy().reshape(-1)\n",
    "    grad = alpha.grad.detach().cpu().numpy().reshape(-1)\n",
    "    importance = alpha_importance.detach().cpu().numpy().reshape(-1)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'source': edge_src,\n",
    "        'target': edge_dst,\n",
    "        'attention': attention,\n",
    "        'grad': grad,\n",
    "        'importance': importance\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "\n",
    "    return df, encoded_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760ba993-9295-4aef-8816-f72e8309b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_node_features(df, encoded_matrix, top_k=5):\n",
    "    biggest_set = [\n",
    "        # Carbon (C) subtypes\n",
    "        'C', 'CA', 'CB', 'CD', 'CD1', 'CD2', 'CE', 'CE1', 'CE2', 'CE3', 'CG', 'CG1', 'CG2', 'CH2', 'CZ', 'CZ2', 'CZ3',\n",
    "\n",
    "        # Oxygen (O) subtypes\n",
    "        'O', 'OH', 'OD1', 'OD2', 'OE1', 'OE2', 'OG', 'OG1',\n",
    "\n",
    "        # Nitrogen (N) subtypes\n",
    "        'N', 'NE', 'NE1', 'NE2', 'ND1', 'ND2', 'NZ', 'NH1', 'NH2',\n",
    "\n",
    "        # Sulfur (S) subtypes\n",
    "        'SD', 'SG'\n",
    "    ]\n",
    "    biggest_set.append('UNKNOWN')\n",
    "\n",
    "    rows = []\n",
    "    for _, row in df.head(top_k).iterrows():\n",
    "        src_idx = int(row['source'])\n",
    "        tgt_idx = int(row['target'])\n",
    "\n",
    "        src_feat = encoded_matrix[src_idx]\n",
    "        tgt_feat = encoded_matrix[tgt_idx]\n",
    "\n",
    "        # Decode one-hot vector back to atom name\n",
    "        def decode_one_hot(vector):\n",
    "            idx = vector.argmax()\n",
    "            return biggest_set[idx] if idx < len(biggest_set) else \"INVALID\"\n",
    "\n",
    "        rows.append({\n",
    "            'source_idx': src_idx,\n",
    "            'target_idx': tgt_idx,\n",
    "            'attention': row['attention'],\n",
    "            'grad': row['grad'],\n",
    "            'importance': row['importance'],\n",
    "            'source_features': src_feat,\n",
    "            'target_features': tgt_feat,\n",
    "            'source_atom': decode_one_hot(src_feat),\n",
    "            'target_atom': decode_one_hot(tgt_feat)\n",
    "        })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2526759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from Bio.PDB import PDBParser\n",
    "\n",
    "# === Helpers ===\n",
    "biggest_set = [\n",
    "    'C', 'CA', 'CB', 'CD', 'CD1', 'CD2', 'CE', 'CE1', 'CE2', 'CE3', 'CG', 'CG1', 'CG2', 'CH2', 'CZ', 'CZ2', 'CZ3',\n",
    "    'O', 'OH', 'OD1', 'OD2', 'OE1', 'OE2', 'OG', 'OG1',\n",
    "    'N', 'NE', 'NE1', 'NE2', 'ND1', 'ND2', 'NZ', 'NH1', 'NH2',\n",
    "    'SD', 'SG', 'UNKNOWN'\n",
    "]\n",
    "\n",
    "def decode_one_hot(vector):\n",
    "    idx = vector.argmax()\n",
    "    return biggest_set[idx] if idx < len(biggest_set) else \"INVALID\"\n",
    "\n",
    "def get_atom_coords_and_residues(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"mol\", pdb_file)\n",
    "    atoms = []\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    if atom.is_disordered():\n",
    "                        # Add ALL conformers (A, B, etc.)\n",
    "                        for alt_atom in atom.disordered_get_list():\n",
    "                            atoms.append(alt_atom)\n",
    "                    else:\n",
    "                        atoms.append(atom)\n",
    "\n",
    "    #print(f\"Total atoms extracted: {len(atoms)}\")\n",
    "    coords = [atom.coord for atom in atoms]\n",
    "    residues = [atom.get_parent().get_resname() for atom in atoms]\n",
    "    return coords, residues\n",
    "\n",
    "def euclidean_distance(coord1, coord2):\n",
    "    return np.linalg.norm(coord1 - coord2)\n",
    "\n",
    "atom_counts_per_exp   = []   # list[Counter]\n",
    "residue_counts_per_exp= []   # list[Counter]\n",
    "distances_per_exp     = []   # list[list[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7309ba41-3e4e-4083-802a-00beda998ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_1.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_2.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_3.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_4.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_5.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_6.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_7.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_8.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_9.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_10.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_11.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_12.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_13.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_14.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/model_bin_15.pth']\n",
      "['../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_1.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_2.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_3.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_4.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_5.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_6.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_7.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_8.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_9.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_10.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_11.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_12.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_13.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_14.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/model_bin_15.pth']\n",
      "['../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_1.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_2.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_3.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_4.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_5.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_6.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_7.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_8.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_9.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_10.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_11.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_12.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_13.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_14.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/model_bin_15.pth']\n",
      "['../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_1.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_2.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_3.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_4.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_5.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_6.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_7.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_8.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_9.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_10.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_11.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_12.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_13.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_14.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/model_bin_15.pth']\n",
      "['../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_1.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_2.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_3.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_4.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_5.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_6.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_7.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_8.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_9.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_10.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_11.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_12.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_13.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_14.pth', '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/model_bin_15.pth']\n"
     ]
    }
   ],
   "source": [
    "# pre load models\n",
    "def load_gat(model_checkpoint, in_channels, out_channels, device):\n",
    "    model = GAT(in_channels, out_channels)\n",
    "    state_dict = torch.load(model_checkpoint, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model = model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "def natural_sort_key(s):\n",
    "    \"\"\"Function to sort strings in a natural alphanumeric order.\"\"\"\n",
    "    return [int(text) if text.isdigit() else text.lower() for text in re.split(r'(\\d+)', s)]\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "all_models = []\n",
    "\n",
    "model_pattern = '../../../Models/Cholesterol/GAT/GATModels-5A_exp1v2/Models/*.pth'\n",
    "model_paths = sorted(glob.glob(model_pattern), key=natural_sort_key)\n",
    "\n",
    "print(model_paths[:15])\n",
    "models_exp1 = [load_gat(p, in_channels=37, out_channels=32, device=device) for p in model_paths]\n",
    "all_models.append(models_exp1)\n",
    "\n",
    "model_pattern = '../../../Models/Cholesterol/GAT/GATModels-5A_exp2v2/Models/*.pth'\n",
    "model_paths = sorted(glob.glob(model_pattern), key=natural_sort_key)\n",
    "\n",
    "print(model_paths[:15])\n",
    "models_exp2 = [load_gat(p, in_channels=37, out_channels=32, device=device) for p in model_paths]\n",
    "all_models.append(models_exp2)\n",
    "\n",
    "model_pattern = '../../../Models/Cholesterol/GAT/GATModels-5A_exp3v2/Models/*.pth'\n",
    "model_paths = sorted(glob.glob(model_pattern), key=natural_sort_key)\n",
    "\n",
    "print(model_paths[:15])\n",
    "models_exp3 = [load_gat(p, in_channels=37, out_channels=32, device=device) for p in model_paths]\n",
    "all_models.append(models_exp3)\n",
    "\n",
    "model_pattern = '../../../Models/Cholesterol/GAT/GATModels-5A_exp4v2/Models/*.pth'\n",
    "model_paths = sorted(glob.glob(model_pattern), key=natural_sort_key)\n",
    "\n",
    "print(model_paths[:15])\n",
    "models_exp4 = [load_gat(p, in_channels=37, out_channels=32, device=device) for p in model_paths]\n",
    "all_models.append(models_exp4)\n",
    "\n",
    "model_pattern = '../../../Models/Cholesterol/GAT/GATModels-5A_exp5v2/Models/*.pth'\n",
    "model_paths = sorted(glob.glob(model_pattern), key=natural_sort_key)\n",
    "\n",
    "print(model_paths[:15])\n",
    "models_exp5 = [load_gat(p, in_channels=37, out_channels=32, device=device) for p in model_paths]\n",
    "all_models.append(models_exp5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8aabe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0 / 211\n",
      "[(32, 46), (46, 58), (24, 46), (17, 46), (22, 46)]\n",
      "PHE TRP 7.3552012 0.04445280471184242 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8CS9-filtered_graphs.npy\n",
      "TRP MET 7.8322554 0.0590613347904336 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8CS9-filtered_graphs.npy\n",
      "MET TRP 9.601895 0.058797344071132004 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8CS9-filtered_graphs.npy\n",
      "VAL TRP 9.067866 0.043162097340941556 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8CS9-filtered_graphs.npy\n",
      "MET TRP 9.429131 0.04246149410973777 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8CS9-filtered_graphs.npy\n",
      "[(11, 32), (17, 32), (11, 17), (32, 49), (23, 32)]\n",
      "GLN CYS 8.996794 0.06287679435736818 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8JXQ-filtered_graphs.npy\n",
      "GLN CYS 10.378705 0.0658341273422858 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8JXQ-filtered_graphs.npy\n",
      "GLN GLN 11.21211 0.059829752287321326 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8JXQ-filtered_graphs.npy\n",
      "CYS TRP 8.619601 0.06663900537950462 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8JXQ-filtered_graphs.npy\n",
      "SER CYS 6.3134093 0.0431400267209594 ../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Test/Positive/8JXQ-filtered_graphs.npy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 37\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_index, model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(experiment_models):\n\u001b[1;32m     31\u001b[0m     df, encoded_matrix \u001b[38;5;241m=\u001b[39m get_important_edges_from_npy(\n\u001b[1;32m     32\u001b[0m         npy_file\u001b[38;5;241m=\u001b[39mnpy_file,\n\u001b[1;32m     33\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     34\u001b[0m         device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[1;32m     35\u001b[0m     )\n\u001b[0;32m---> 37\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[43mget_edge_node_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rows:\n\u001b[1;32m     40\u001b[0m         u \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource_idx\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36mget_edge_node_features\u001b[0;34m(df, encoded_matrix, top_k)\u001b[0m\n\u001b[1;32m     15\u001b[0m biggest_set\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUNKNOWN\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m rows \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mhead(top_k)\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     19\u001b[0m     src_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     20\u001b[0m     tgt_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/core/frame.py:1559\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1557\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m-> 1559\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1560\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[1;32m   1561\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bio-ml/lib/python3.10/site-packages/pandas/core/series.py:398\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    391\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    396\u001b[0m     fastpath: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m lib\u001b[38;5;241m.\u001b[39mNoDefault \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mno_default,\n\u001b[1;32m    397\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 398\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fastpath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    399\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    400\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m keyword in pd.Series is deprecated and will \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe removed in a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    402\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    403\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    404\u001b[0m         )\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for exp_index, experiment_models in enumerate(all_models):\n",
    "    atom_subtype_counter = Counter()\n",
    "    residue_type_counter = Counter()\n",
    "    distance_list = []\n",
    "    edge_records = []  # put this BEFORE your big loops\n",
    "\n",
    "    npy_files = glob.glob(f'../../../Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp{exp_index + 1}/Test/Positive/*.npy')\n",
    "    npy_files.extend(glob.glob(f'../../../Data/SplitData/Cholesterol/IvanTestSet/ivan-separate-graphs-5A/positive/*.npy'))\n",
    "    for index, npy_file in enumerate(npy_files):\n",
    "        if index % 10 == 0:\n",
    "            print(\"Progress:\", index, \"/\", len(npy_files))\n",
    "        #npy_file = \"../../../Data/SplitData/Cholesterol/IvanTestSet/ivan-separate-graphs-5A/positive/7KY5-filtered_graphs.npy\"\n",
    "        basename = os.path.basename(npy_file)\n",
    "        if \"ivan\" in npy_file:\n",
    "            pdb_file = f\"../GNN/ivan-pdbs-distinct-5A/positive/{basename[:4]}-filtered.pdb\"\n",
    "        else:\n",
    "            pdb_file = f\"../GNN/filtered-pdbs-distinct-5A/positive/{basename[:4]}-filtered.pdb\"\n",
    "\n",
    "        edge_counts = Counter()  # key: (min_idx, max_idx) -> count\n",
    "        # Keep stats for tie-breaks and metadata\n",
    "        edge_stats = defaultdict(lambda: {\n",
    "            \"importances\": [],\n",
    "            \"grads\": [],\n",
    "            \"src_atom\": None,\n",
    "            \"tgt_atom\": None,\n",
    "        })\n",
    "\n",
    "        coords, residues = get_atom_coords_and_residues(pdb_file)\n",
    "\n",
    "        for model_index, model in enumerate(experiment_models):\n",
    "            df, encoded_matrix = get_important_edges_from_npy(\n",
    "                npy_file=npy_file,\n",
    "                model=model,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            rows = get_edge_node_features(df, encoded_matrix, top_k=1000)\n",
    "\n",
    "            for r in rows:\n",
    "                u = int(r['source_idx'])\n",
    "                v = int(r['target_idx'])\n",
    "                a, b = (u, v) if u < v else (v, u)  # undirected canonical key\n",
    "                edge_counts[(a, b)] += 1\n",
    "\n",
    "                # collect stats for tie-breaking & later reporting\n",
    "                s = edge_stats[(a, b)]\n",
    "                s[\"importances\"].append(float(r.get(\"importance\", 0.0)))\n",
    "                s[\"grads\"].append(float(r.get(\"grad\", 0.0)))\n",
    "                # store atom subtypes (last write wins; these should be consistent per node)\n",
    "                s[\"src_atom\"] = r.get(\"source_atom\", s[\"src_atom\"])\n",
    "                s[\"tgt_atom\"] = r.get(\"target_atom\", s[\"tgt_atom\"])\n",
    "        if edge_counts:\n",
    "            # ---------- compute per-sample mean importances over ALL candidate edges ----------\n",
    "            # mean importance for every (a,b) this sample produced across models\n",
    "            mean_imp_per_edge = {}\n",
    "            for k, s in edge_stats.items():\n",
    "                imps = s[\"importances\"]\n",
    "                mean_imp_per_edge[k] = float(np.mean(imps)) if imps else 0.0\n",
    "\n",
    "            # sample-wise min-max params\n",
    "            all_means = np.array(list(mean_imp_per_edge.values()), dtype=float)\n",
    "            imp_min = float(np.min(all_means)) if all_means.size else 0.0\n",
    "            imp_max = float(np.max(all_means)) if all_means.size else 0.0\n",
    "            range_eps = imp_max - imp_min\n",
    "\n",
    "            def norm_imp(raw):\n",
    "                # normalize to [0,1] per sample\n",
    "                if range_eps <= 1e-12:\n",
    "                    # degenerate case: all edges have same mean importance\n",
    "                    # put them at 1.0 to avoid zeroing signal\n",
    "                    return 1.0\n",
    "                return (raw - imp_min) / range_eps\n",
    "\n",
    "            # sort keys by (-count, -mean_importance)\n",
    "            def sort_key(k):\n",
    "                cnt = edge_counts[k]\n",
    "                imps = edge_stats[k][\"importances\"]\n",
    "                mean_imp = float(np.mean(imps)) if len(imps) > 0 else 0.0\n",
    "                return (-cnt, -mean_imp)\n",
    "\n",
    "            top_keys = sorted(edge_counts.keys(), key=sort_key)[:5]\n",
    "            print(top_keys)\n",
    "\n",
    "            # Only now update your counters and edge_records with these 5 edges (once per edge)\n",
    "            for (a, b) in top_keys:\n",
    "                # safe lookups\n",
    "                src = a\n",
    "                tgt = b\n",
    "\n",
    "                src_res = residues[src] if src < len(residues) else \"UNK\"\n",
    "                tgt_res = residues[tgt] if tgt < len(residues) else \"UNK\"\n",
    "\n",
    "                # distance is fixed for this sample (same coords), so compute directly\n",
    "                dist = euclidean_distance(np.array(coords[src]), np.array(coords[tgt]))\n",
    "\n",
    "                # representative atom subtypes from stats (if available)\n",
    "                src_atom = edge_stats[(a, b)][\"src_atom\"] if edge_stats[(a, b)][\"src_atom\"] is not None else \"UNK\"\n",
    "                tgt_atom = edge_stats[(a, b)][\"tgt_atom\"] if edge_stats[(a, b)][\"tgt_atom\"] is not None else \"UNK\"\n",
    "\n",
    "                # mean importance (optional but handy to store)\n",
    "                mean_importance_raw = mean_imp_per_edge.get((a, b), 0.0)\n",
    "                mean_importance_norm = norm_imp(mean_importance_raw)\n",
    "\n",
    "                if src_res == \"ILE\" and tgt_res == \"ILE\" and dist > 3.0:\n",
    "                    print(src_atom, \"is source\", tgt_atom, \"is target\", pdb_file, \"is pdb\", src, \"is atom id for source\", tgt, \"is atom id for target\", dist, \"is distance\")\n",
    "\n",
    "                # Update counters ONCE per selected edge (not multiplied by frequency)\n",
    "                atom_subtype_counter[src_atom] += 1\n",
    "                atom_subtype_counter[tgt_atom] += 1\n",
    "                residue_type_counter[src_res] += 1\n",
    "                residue_type_counter[tgt_res] += 1\n",
    "                distance_list.append(dist)\n",
    "\n",
    "                print(src_res, tgt_res, dist, mean_importance_norm, npy_file)\n",
    "\n",
    "                # Save record\n",
    "                edge_records.append({\n",
    "                    \"src_atom\": src_atom,\n",
    "                    \"tgt_atom\": tgt_atom,\n",
    "                    \"src_res\": src_res,\n",
    "                    \"tgt_res\": tgt_res,\n",
    "                    \"importance\": mean_importance_norm,  # averaged across models for this sample\n",
    "                    \"distance\": float(dist),\n",
    "                    \"frequency\": int(edge_counts[(a, b)]),  # how many models surfaced this edge\n",
    "                    \"sample_file\": basename\n",
    "                })\n",
    "        # ------------------------------------------------------\n",
    "\n",
    "    atom_counts_per_exp.append(atom_subtype_counter)\n",
    "    residue_counts_per_exp.append(residue_type_counter)\n",
    "    distances_per_exp.append(distance_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caacf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===== Helpers to save/load =====\n",
    "def save_counters_to_csv(counters, filename):\n",
    "    \"\"\"\n",
    "    counters: list of Counter objects (one per experiment)\n",
    "    Saves to CSV with __experiment__ column + one column per key\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(counters).fillna(0).astype(int)\n",
    "    df.insert(0, \"__experiment__\", np.arange(1, len(df)+1))\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_counters_from_csv(filename):\n",
    "    \"\"\"\n",
    "    Loads CSV back into list of Counter dicts\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    counters = []\n",
    "    for _, row in df.iterrows():\n",
    "        c = {col: int(row[col]) for col in df.columns if col != \"__experiment__\"}\n",
    "        counters.append(c)\n",
    "    return counters\n",
    "\n",
    "def save_hists_to_csv(hists, bin_edges, filename):\n",
    "    \"\"\"\n",
    "    hists: list of per-experiment hist arrays\n",
    "    bin_edges: numpy array of bin edges\n",
    "    Saves counts with __experiment__ + bin columns labeled by bin center\n",
    "    \"\"\"\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "    df = pd.DataFrame(hists, columns=[f\"bin_{c:.1f}\" for c in bin_centers])\n",
    "    df.insert(0, \"__experiment__\", np.arange(1, len(df)+1))\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "def load_hists_from_csv(filename):\n",
    "    \"\"\"\n",
    "    Loads histogram CSV back into list of arrays + bin centers\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename)\n",
    "    bin_cols = [c for c in df.columns if c != \"__experiment__\"]\n",
    "    hists = df[bin_cols].to_numpy()\n",
    "    bin_centers = [float(c.replace(\"bin_\", \"\")) for c in bin_cols]\n",
    "    return hists, np.array(bin_centers)\n",
    "\n",
    "# ===== Example usage with your data =====\n",
    "# Save\n",
    "# save_counters_to_csv(atom_counts_per_exp, \"CSV\\'s/gat_external_atom_counts_per_exp.csv\")\n",
    "# save_counters_to_csv(residue_counts_per_exp, \"CSV\\'s/gat_external_residue_counts_per_exp.csv\")\n",
    "# #save_hists_to_csv(hists, bin_edges, \"distance_hists.csv\")\n",
    "\n",
    "# # Load\n",
    "# atom_counts_per_exp = load_counters_from_csv(\"CSV\\'s/gat_external_atom_counts_per_exp.csv\")\n",
    "# residue_counts_per_exp = load_counters_from_csv(\"CSV\\'s/gat_external_residue_counts_per_exp.csv\")\n",
    "# hists, bin_centers = load_hists_from_csv(\"distance_hists.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca87f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== helper to compute mean/std across experiments for categorical bars =====\n",
    "def mean_std_bars(counters, top_n=None):\n",
    "    # union of all keys across experiments\n",
    "    labels = sorted(set().union(*[c.keys() for c in counters]))\n",
    "    mat = np.array([[c.get(lbl, 0) for lbl in labels] for c in counters], dtype=float)\n",
    "    means = mat.mean(axis=0)\n",
    "    stds  = mat.std(axis=0, ddof=1) if mat.shape[0] > 1 else np.zeros_like(means)\n",
    "    order = np.argsort(-means)  # sort by mean desc\n",
    "\n",
    "    if top_n is not None:\n",
    "        order = order[:top_n]\n",
    "\n",
    "    return [labels[i] for i in order], means[order], stds[order]\n",
    "\n",
    "# ===== 1) Atom subtype: mean ± std =====\n",
    "atom_labels, atom_means, atom_stds = mean_std_bars(atom_counts_per_exp)\n",
    "plt.figure(figsize=(11, 5))\n",
    "x = np.arange(len(atom_labels))\n",
    "plt.bar(x, atom_means, yerr=atom_stds, capsize=4)\n",
    "plt.xticks(x, atom_labels, rotation=90)\n",
    "plt.title(\"Atom Subtype Frequency (Mean ± Std across 5 experiments)\")\n",
    "plt.xlabel(\"Atom Subtype\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 2) Residue type: mean ± std =====\n",
    "res_labels, res_means, res_stds = mean_std_bars(residue_counts_per_exp)\n",
    "plt.figure(figsize=(10, 5))\n",
    "x = np.arange(len(res_labels))\n",
    "plt.bar(x, res_means, yerr=res_stds, capsize=4)\n",
    "plt.xticks(x, res_labels, rotation=45, ha='right')\n",
    "plt.title(\"Residue Type Frequency (Mean ± Std across 5 experiments)\")\n",
    "plt.xlabel(\"Residue Type\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ===== 3) Distance histogram: mean ± std per bin =====\n",
    "# use a fixed bin width (2 Å to match your ticks)\n",
    "bin_width = 2.0\n",
    "all_dists = np.concatenate([np.array(d) for d in distances_per_exp]) if distances_per_exp else np.array([0.0])\n",
    "max_d = math.ceil(all_dists.max()) if all_dists.size else 0\n",
    "# ensure at least one bin\n",
    "max_edge = max(bin_width, math.ceil(max_d / bin_width) * bin_width)\n",
    "bin_edges = np.arange(0.0, max_edge + bin_width + 1e-9, bin_width)\n",
    "\n",
    "# per-experiment hist counts with same bins\n",
    "hists = []\n",
    "for dlist in distances_per_exp:\n",
    "    hist, _ = np.histogram(dlist, bins=bin_edges)\n",
    "    hists.append(hist.astype(float))\n",
    "H = np.vstack(hists) if hists else np.zeros((1, len(bin_edges)-1))\n",
    "hist_means = H.mean(axis=0)\n",
    "hist_stds  = H.std(axis=0, ddof=1) if H.shape[0] > 1 else np.zeros_like(hist_means)\n",
    "\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(bin_centers, hist_means, width=bin_width*0.9, yerr=hist_stds, capsize=4, align='center', edgecolor='black')\n",
    "plt.title(\"Edge Distance Distribution (Mean ± Std across 5 experiments)\")\n",
    "plt.xlabel(\"Distance (Å)\")\n",
    "plt.ylabel(\"Number of Edges\")\n",
    "plt.xticks(np.arange(0, max_edge + 1e-9, 2.0))  # tick every 2 Å like before\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b59478",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_df = pd.DataFrame(edge_records)\n",
    "if edges_df.empty:\n",
    "    raise ValueError(\"No edges collected; edge_records is empty.\")\n",
    "\n",
    "filtered_df = edges_df[edges_df[\"distance\"] >= 4]\n",
    "\n",
    "# Sort by importance descending and get top 10\n",
    "top_edges = filtered_df.sort_values(\"importance\", ascending=False)\n",
    "#print(\"Top 10 edges by importance:\")\n",
    "#print(top_edges[[\"src_res\", \"tgt_res\", \"src_atom\", \"tgt_atom\", \"importance\", \"distance\", \"sample_file\"]].to_string(index=False))\n",
    "\n",
    "if filtered_df.empty:\n",
    "    raise ValueError(\"filtered_df is empty (no edges at distance ≥ 3).\")\n",
    "\n",
    "df = filtered_df.copy()\n",
    "\n",
    "# 1) Treat residue connections as undirected pairs\n",
    "df[\"res_a\"] = df.apply(lambda x: min(x[\"src_res\"], x[\"tgt_res\"]), axis=1)\n",
    "df[\"res_b\"] = df.apply(lambda x: max(x[\"src_res\"], x[\"tgt_res\"]), axis=1)\n",
    "df[\"pair\"]  = df[\"res_a\"] + \"-\" + df[\"res_b\"]\n",
    "\n",
    "# ----- Distance binning -----\n",
    "bin_edges  = [4, 8, 12, 16, 20, np.inf]\n",
    "bin_labels = [\"4–<8 Å\", \"8–<12 Å\", \"12–<16 Å\", \"16–<20 Å\", \"≥20 Å\"]\n",
    "df[\"dist_bin\"] = pd.cut(df[\"distance\"], bins=bin_edges, labels=bin_labels, right=False)\n",
    "df[\"pair_bin\"] = df[\"pair\"] + \" | \" + df[\"dist_bin\"].astype(str)\n",
    "\n",
    "# Define the importance threshold\n",
    "cutoff = 0.30\n",
    "df[\"is_important\"] = df[\"importance\"] >= cutoff\n",
    "\n",
    "# ----- Aggregations per (pair, bin) -----\n",
    "pair_bin_stats = (\n",
    "    df.groupby([\"pair\", \"dist_bin\"])\n",
    "      .agg(\n",
    "          count             = (\"importance\", \"size\"),\n",
    "          total_importance  = (\"importance\", \"sum\"),\n",
    "          avg_importance    = (\"importance\", \"mean\"),\n",
    "          max_importance    = (\"importance\", \"max\"),\n",
    "          count_importance=(\"is_important\", \"sum\"),               # ≥ 0.7\n",
    "          #count_unimportant=(\"is_important\", lambda x: (~x).sum())  # < 0.7\n",
    "      )\n",
    "      .rename(columns={\"count_importance\": \"count_importance>0.3\"})\n",
    "      .sort_values([\"total_importance\", \"avg_importance\", \"count\"], ascending=False)\n",
    ")\n",
    "\n",
    "print(\"\\n=== Residue Pair Instances by Distance Bin (ranked) ===\")\n",
    "print(pair_bin_stats.to_string())\n",
    "\n",
    "# --- Normalize total_importance to [0, 1] for coloring ---\n",
    "pbs = pair_bin_stats.reset_index()\n",
    "tmin = pbs[\"total_importance\"].min()\n",
    "tmax = pbs[\"total_importance\"].max()\n",
    "if tmax > tmin:\n",
    "    pbs[\"color_val\"] = (pbs[\"total_importance\"] - tmin) / (tmax - tmin)\n",
    "else:\n",
    "    # all totals equal (or only one); color them all as 1.0\n",
    "    pbs[\"color_val\"] = 1.0\n",
    "\n",
    "# --- Pick only the highest edge per (pair, dist_bin) ---\n",
    "# tie-breakers: highest importance, then shortest distance (optional), then first\n",
    "best_edges = (\n",
    "    df.sort_values([\"pair\",\"dist_bin\",\"importance\",\"distance\"],\n",
    "                   ascending=[True, True, False, True])\n",
    "      .groupby([\"pair\",\"dist_bin\"], as_index=False)\n",
    "      .head(1)\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "# --- Attach normalized color to each chosen edge ---\n",
    "best_edges = best_edges.merge(\n",
    "    pbs[[\"pair\",\"dist_bin\",\"color_val\",\"total_importance\"]],\n",
    "    on=[\"pair\",\"dist_bin\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# --- Columns expected by your plotter ---\n",
    "best_edges[\"importance_val\"] = best_edges[\"importance\"]\n",
    "best_edges[\"distance_val\"]   = best_edges[\"distance\"]\n",
    "\n",
    "# ----- (Optional) Ensure every pair is listed for EVERY bin (zero-fill for missing bins) -----\n",
    "all_pairs = df[\"pair\"].unique()\n",
    "full_index = pd.MultiIndex.from_product([all_pairs, bin_labels], names=[\"pair\", \"dist_bin\"])\n",
    "pair_bin_stats_full = pair_bin_stats.reindex(full_index, fill_value=0)\n",
    "\n",
    "print(\"\\n=== Residue Pair Instances by Distance Bin (full grid, zero-filled) ===\")\n",
    "print(pair_bin_stats_full.to_string())\n",
    "\n",
    "# ----- (Optional) Pivot to a compact table: counts per bin for each pair -----\n",
    "pair_bin_counts = (\n",
    "    pair_bin_stats_full[\"count\"]\n",
    "      .unstack(\"dist_bin\")\n",
    "      .fillna(0)\n",
    "      .astype(int)\n",
    "      .loc[  # order pairs by total importance across all bins\n",
    "          pair_bin_stats_full.groupby(level=0)[\"total_importance\"].sum().sort_values(ascending=False).index\n",
    "      ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Counts per Distance Bin (rows = residue pair) ===\")\n",
    "print(pair_bin_counts.to_string())\n",
    "\n",
    "# ----- (Optional) Merge pair-level ranking with per-bin counts for one table -----\n",
    "pair_level_rank = (\n",
    "    df.groupby(\"pair\")\n",
    "      .agg(count=(\"importance\",\"size\"),\n",
    "           total_importance=(\"importance\",\"sum\"),\n",
    "           avg_importance=(\"importance\",\"mean\"))\n",
    "      .sort_values([\"total_importance\",\"avg_importance\",\"count\"], ascending=False)\n",
    ")\n",
    "\n",
    "merged_pair_summary = pair_level_rank.join(pair_bin_counts, how=\"left\").fillna(0)\n",
    "print(\"\\n=== Pair Ranking + Counts per Distance Bin ===\")\n",
    "print(merged_pair_summary.to_string())\n",
    "\n",
    "# Treat residue pairs as undirected, but keep all rows (no aggregation)\n",
    "edges_df[\"res_a\"] = edges_df.apply(lambda x: min(x[\"src_res\"], x[\"tgt_res\"]), axis=1)\n",
    "edges_df[\"res_b\"] = edges_df.apply(lambda x: max(x[\"src_res\"], x[\"tgt_res\"]), axis=1)\n",
    "\n",
    "# For plotting, we'll use the raw per-edge values\n",
    "edges_df[\"importance_val\"] = edges_df[\"importance\"]\n",
    "edges_df[\"distance_val\"]   = edges_df[\"distance\"]\n",
    "\n",
    "def plot_residue_chord_raw(\n",
    "    df,\n",
    "    importance_col=\"importance_val\",   # still used for edge width\n",
    "    distance_col=\"distance_val\",\n",
    "    color_col=\"color_val\",             # <-- NEW: use normalized total_importance for color\n",
    "    top_k=50,\n",
    "    node_order=None,\n",
    "    figsize=(14, 14),\n",
    "    title_size=26,\n",
    "    node_label_size=36,\n",
    "    distance_fontsize=26,\n",
    "    tick_label_size=30,\n",
    "    cbar_label_size=40,\n",
    "    edge_alpha=0.4,\n",
    "    curvature=0.55,\n",
    "    cmap_name=\"viridis\",\n",
    "    pad=0.45,\n",
    "    node_inset=0.94,\n",
    "    label_inset=0.97,\n",
    "    clip_edges=True,\n",
    "    dist_min=4,\n",
    "    dist_max=None,\n",
    "    label_top_n=20,\n",
    "):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.path import Path\n",
    "    from matplotlib.patches import PathPatch, Circle\n",
    "    from matplotlib import colors\n",
    "\n",
    "    rc = {\n",
    "        \"font.size\": tick_label_size,\n",
    "        \"axes.titlesize\": title_size,\n",
    "        \"axes.labelsize\": tick_label_size,\n",
    "        \"xtick.labelsize\": tick_label_size,\n",
    "        \"ytick.labelsize\": tick_label_size,\n",
    "        \"legend.fontsize\": tick_label_size,\n",
    "        \"figure.titlesize\": title_size,\n",
    "    }\n",
    "\n",
    "    # Distance filters (operate on the column passed in)\n",
    "    if dist_min is not None:\n",
    "        df = df[df[distance_col] >= dist_min]\n",
    "    if dist_max is not None:\n",
    "        df = df[df[distance_col] <= dist_max]\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No edges to plot after distance filter.\")\n",
    "\n",
    "    # Only keep top_k rows (data already pre-filtered to best per pair×bin)\n",
    "    df = df.sort_values(importance_col, ascending=False).head(top_k).copy()\n",
    "    label_indices = set(df.head(min(label_top_n, len(df))).index)\n",
    "\n",
    "    nodes = node_order or sorted(set(df[\"res_a\"]).union(set(df[\"res_b\"])))\n",
    "    n = len(nodes)\n",
    "    node_to_idx = {r: i for i, r in enumerate(nodes)}\n",
    "    angles = np.linspace(0, 2*np.pi, n, endpoint=False)\n",
    "    R = 1.0\n",
    "    coords = np.c_[R*np.cos(angles), R*np.sin(angles)]\n",
    "\n",
    "    # --- Color mapping from normalized color_col in [0,1] ---\n",
    "    # If missing or degenerate, fall back to zeros.\n",
    "    color_vals = (df[color_col] if color_col in df.columns\n",
    "              else pd.Series(0.0, index=df.index)).to_numpy().astype(float)\n",
    "    color_vals = np.clip(color_vals, 0.0, 1.0)\n",
    "    norm = colors.Normalize(vmin=0.0, vmax=1.0)\n",
    "    cmap = plt.get_cmap(cmap_name)\n",
    "\n",
    "    # Edge width still based on per-edge importance (can change if you want)\n",
    "    imp_vals = df[importance_col].to_numpy().astype(float)\n",
    "    imp_min, imp_max = np.percentile(imp_vals, 2), np.percentile(imp_vals, 98)\n",
    "    if imp_min == imp_max:\n",
    "        imp_min, imp_max = imp_vals.min(), imp_vals.max()\n",
    "    if imp_min == imp_max:\n",
    "        imp_min, imp_max = 0.0, 1.0\n",
    "\n",
    "    def width_fn(v, mn=0.5, mx=6.0):\n",
    "        return mn + (mx - mn) * ((v - imp_min) / (imp_max - imp_min) if imp_max > imp_min else 0.5)\n",
    "\n",
    "    def color_fn(v):\n",
    "        return cmap(norm(v))  # v is already 0..1\n",
    "\n",
    "    with plt.rc_context(rc):\n",
    "        fig, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        # nodes + labels\n",
    "        label_r = 1.08\n",
    "        for i, r in enumerate(nodes):\n",
    "            x, y = coords[i]\n",
    "            ax.plot([x], [y], \"o\", ms=7, color=\"black\")\n",
    "            lx, ly = label_r*np.cos(angles[i]), label_r*np.sin(angles[i])\n",
    "            ha = \"left\" if lx >= 0 else \"right\"\n",
    "            ax.text(lx, ly, r, ha=ha, va=\"center\", fontsize=node_label_size)\n",
    "\n",
    "        clip_circle = Circle((0, 0), 1.0, transform=ax.transData) if clip_edges else None\n",
    "\n",
    "        def _quad(p0, p1, p2, t):\n",
    "            return (1-t)**2*p0 + 2*(1-t)*t*p1 + t**2*p2\n",
    "\n",
    "        # draw edges\n",
    "        for idx, row in df.iterrows():\n",
    "            a = node_to_idx[row[\"res_a\"]]; b = node_to_idx[row[\"res_b\"]]\n",
    "            show_label = idx in label_indices\n",
    "            w = width_fn(float(row[importance_col]))\n",
    "            c = color_fn(float(row[color_col]))\n",
    "\n",
    "            if a == b:\n",
    "                p = coords[a] * node_inset\n",
    "                t = p[::-1] * np.array([-1, 1]); t = t / (np.linalg.norm(t) + 1e-12)\n",
    "                loop_width = 0.12; loop_curv = 0.70\n",
    "                p0 = p + t * loop_width\n",
    "                p2 = p - t * loop_width\n",
    "                p1 = (1.0 - loop_curv) * p\n",
    "                path  = Path(np.vstack([p0, p1, p2]), [Path.MOVETO, Path.CURVE3, Path.CURVE3])\n",
    "                patch = PathPatch(path, lw=w, edgecolor=c, facecolor=\"none\",\n",
    "                                  alpha=edge_alpha, capstyle=\"round\", clip_on=True)\n",
    "                if clip_circle is not None: patch.set_clip_path(clip_circle)\n",
    "                ax.add_patch(patch)\n",
    "                if show_label:\n",
    "                    mid_xy = (p + t * (loop_width * 0.3)) * label_inset\n",
    "                    txt = ax.text(mid_xy[0], mid_xy[1], f'{row[distance_col]:.1f} Å',\n",
    "                                  fontsize=distance_fontsize, ha=\"center\", va=\"center\",\n",
    "                                  bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
    "                    if clip_circle is not None: txt.set_clip_path(clip_circle)\n",
    "                continue\n",
    "\n",
    "            p0 = coords[a]*node_inset\n",
    "            p2 = coords[b]*node_inset\n",
    "            p_mid = 0.5*(p0+p2)\n",
    "            p1 = (1.0-curvature)*p_mid\n",
    "            path = Path(np.vstack([p0, p1, p2]), [Path.MOVETO, Path.CURVE3, Path.CURVE3])\n",
    "            patch = PathPatch(path, lw=w, edgecolor=c, facecolor=\"none\",\n",
    "                              alpha=edge_alpha, capstyle=\"round\", clip_on=True)\n",
    "            if clip_circle is not None: patch.set_clip_path(clip_circle)\n",
    "            ax.add_patch(patch)\n",
    "\n",
    "            if show_label:\n",
    "                mid_xy = _quad(p0, p1, p2, 0.5) * label_inset\n",
    "                txt = ax.text(mid_xy[0], mid_xy[1], f'{row[distance_col]:.1f} Å',\n",
    "                              fontsize=distance_fontsize, ha=\"center\", va=\"center\",\n",
    "                              bbox=dict(boxstyle=\"round,pad=0.25\", fc=\"white\", ec=\"none\", alpha=0.7))\n",
    "                if clip_circle is not None: txt.set_clip_path(clip_circle)\n",
    "\n",
    "        # colorbar for normalized total_importance\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm); sm.set_array([])\n",
    "        cbar = plt.colorbar(sm, ax=ax, orientation=\"horizontal\", fraction=0.05, pad=0.12, aspect=40)\n",
    "        cbar.set_label('Total importance (pair×distance bin), normalized', fontsize=cbar_label_size)\n",
    "        cbar.ax.tick_params(labelsize=tick_label_size)\n",
    "\n",
    "        lim = 1.15\n",
    "        ax.set_xlim(-lim, lim); ax.set_ylim(-lim, lim)\n",
    "        fig.subplots_adjust(left=0.05, right=0.95, top=0.95, bottom=0.18)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        return fig, ax\n",
    "\n",
    "\n",
    "fig, ax = plot_residue_chord_raw(\n",
    "    best_edges,\n",
    "    importance_col=\"importance_val\",   # widths\n",
    "    distance_col=\"distance_val\",       # labels\n",
    "    color_col=\"color_val\",             # colors from normalized total_importance\n",
    "    top_k=40            # plot all chosen edges\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe8de53-889e-4163-8465-3db7f1100dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# df.to_csv(\"6PS5_GAT.csv\")\n",
    "df = pd.read_csv(\"6PS5_GAT.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b2c744-f158-4cd8-ae7b-7831f0292302",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = get_edge_node_features(df, encoded_matrix, top_k=50)\n",
    "\n",
    "for r in rows:\n",
    "    print(f\"\\nEdge: {r['source_idx']} -> {r['target_idx']}\")\n",
    "    print(\"Source:\", r['source_features'], \"Atom:\", r['source_atom'])\n",
    "    print(\"Target:\", r['target_features'], \"Atom:\", r['target_atom'])\n",
    "    print(\"Importance:\", r['importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33131106-c634-42cb-aac0-259972abbd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "\n",
    "# Load the structure\n",
    "pdb_file = \"../GNN/filtered-pdbs-distinct-5A/positive/6PS5-filtered.pdb\"\n",
    "parser = PDBParser(QUIET=True)\n",
    "structure = parser.get_structure(\"structure_id\", pdb_file)\n",
    "\n",
    "# Get all atoms\n",
    "atoms = list(structure.get_atoms())\n",
    "\n",
    "# Check enough atoms exist\n",
    "if len(atoms) >= 54:\n",
    "    atom1 = atoms[35]  # 36th atom\n",
    "    atom2 = atoms[47]  # 54th atom\n",
    "\n",
    "    coord1 = atom1.coord\n",
    "    coord2 = atom2.coord\n",
    "\n",
    "    # Print atom 1 info\n",
    "    print(\"Atom 1:\")\n",
    "    print(\"  Name:\", atom1.get_name())\n",
    "    print(\"  Element:\", atom1.element)\n",
    "    print(\"  Coordinates:\", coord1)\n",
    "    print(\"  Residue:\", atom1.get_parent().get_resname())\n",
    "    print(\"  Chain ID:\", atom1.get_parent().get_parent().id)\n",
    "    print(\"  Residue ID:\", atom1.get_parent().id)\n",
    "\n",
    "    # Print atom 2 info\n",
    "    print(\"\\nAtom 2:\")\n",
    "    print(\"  Name:\", atom2.get_name())\n",
    "    print(\"  Element:\", atom2.element)\n",
    "    print(\"  Coordinates:\", coord2)\n",
    "    print(\"  Residue:\", atom2.get_parent().get_resname())\n",
    "    print(\"  Chain ID:\", atom2.get_parent().get_parent().id)\n",
    "    print(\"  Residue ID:\", atom2.get_parent().id)\n",
    "\n",
    "    # Compute distance\n",
    "    distance = np.linalg.norm(coord1 - coord2)\n",
    "    print(f\"\\nEuclidean distance between atom 36 and atom 54: {distance:.3f} Å\")\n",
    "\n",
    "else:\n",
    "    print(\"PDB file has fewer than 54 atoms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193c3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_files = glob.glob(\"Notebooks/Cholesterol/GNN/ivan-pdbs-distinct-5A/positive/*.pdb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
