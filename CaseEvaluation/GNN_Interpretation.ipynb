{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e70b4d6-e92e-4937-a068-33c473004172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, gc, csv\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import matplotlib.cm as cm\n",
    "from Bio.PDB import PDBParser\n",
    "from collections import Counter\n",
    "\n",
    "# ================================ HELPERS ==============================\n",
    "def normalize01(a: np.ndarray) -> np.ndarray:\n",
    "    a = a.astype(np.float32, copy=False)\n",
    "    mn, mx = float(a.min()), float(a.max())\n",
    "    if mx - mn < 1e-8:\n",
    "        return np.zeros_like(a, dtype=np.float32)\n",
    "    return (a - mn) / (mx - mn)\n",
    "\n",
    "def save_png_gray(arr: np.ndarray, path: str):\n",
    "    Image.fromarray((normalize01(arr)*255).astype(np.uint8)).save(path)\n",
    "\n",
    "def save_overlay(inp: np.ndarray, cam: np.ndarray, path: str):\n",
    "    inp01 = normalize01(inp)\n",
    "    cam01 = normalize01(cam)\n",
    "    inp_rgb  = (np.stack([inp01]*3, -1)*255).astype(np.uint8)\n",
    "    heatmap  = (cm.jet(cam01)[..., :3]*255).astype(np.uint8)\n",
    "    overlay  = (0.6*inp_rgb + 0.4*heatmap).clip(0,255).astype(np.uint8)\n",
    "    Image.fromarray(overlay).save(path)\n",
    "\n",
    "def save_line_plot_pil(values, out_path, title=\"\", xlabel=\"\", ylabel=\"\", size=(1000, 360), margin=60):\n",
    "    values = np.asarray(values, dtype=np.float32)\n",
    "    W,H = size\n",
    "    img = Image.new(\"RGB\", (W,H), \"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    left, top = margin, margin\n",
    "    right, bottom = W - margin, H - margin\n",
    "    # axes\n",
    "    draw.line([(left,bottom),(right,bottom)], fill=\"black\", width=2)\n",
    "    draw.line([(left,top),(left,bottom)], fill=\"black\", width=2)\n",
    "    if values.size > 0:\n",
    "        vmin, vmax = float(values.min()), float(values.max())\n",
    "        norm = (values - vmin) / max(vmax - vmin, 1e-8)\n",
    "        xs = np.linspace(left, right, len(values))\n",
    "        ys = bottom - norm * (bottom - top)\n",
    "        for i in range(len(xs)-1):\n",
    "            draw.line([(xs[i], ys[i]), (xs[i+1], ys[i+1])], fill=(30,144,255), width=2)\n",
    "    try:\n",
    "        font = ImageFont.load_default()\n",
    "        if title:  draw.text((left, top-25), title, fill=\"black\", font=font)\n",
    "        if xlabel: draw.text((left + (right-left)//2 - 40, bottom + 8), xlabel, fill=\"black\", font=font)\n",
    "        if ylabel: draw.text((8, top - 10), ylabel, fill=\"black\", font=font)\n",
    "    except: pass\n",
    "    img.save(out_path, format=\"PNG\")\n",
    "\n",
    "def save_bar_plot_pil(values, labels, out_path, title=\"\", size=(1200, 500), margin=80, color=(30,144,255)):\n",
    "    values = np.asarray(values, dtype=np.float32)\n",
    "    n = len(values); W,H = size\n",
    "    img = Image.new(\"RGB\", (W,H), \"white\")\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    left, top = margin, margin\n",
    "    right, bottom = W - margin, H - margin\n",
    "    plot_w, plot_h = right-left, bottom-top\n",
    "    try:\n",
    "        font = ImageFont.load_default()\n",
    "        if title: draw.text((left, top-25), title, fill=\"black\", font=font)\n",
    "    except: pass\n",
    "    if n == 0:\n",
    "        img.save(out_path); return\n",
    "    vmax = float(values.max()); scale = plot_w / max(vmax, 1e-8)\n",
    "    bar_h = plot_h / max(n,1)\n",
    "    for i, v in enumerate(values):\n",
    "        y0 = top + i*bar_h + 4; y1 = top + (i+1)*bar_h - 4; x1 = left + v*scale\n",
    "        draw.rectangle([left, y0, x1, y1], fill=color)\n",
    "        lbl = str(labels[i])[:22] + (\"…\" if len(str(labels[i]))>22 else \"\")\n",
    "        try:\n",
    "            draw.text((5, (y0+y1)/2 - 6), lbl, fill=\"black\", font=font)\n",
    "            draw.text((x1 + 5, (y0+y1)/2 - 6), f\"{v:.3f}\", fill=\"black\", font=font)\n",
    "        except: pass\n",
    "    img.save(out_path, format=\"PNG\")\n",
    "\n",
    "def load_sample(path, exp_index):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      M_in: np.ndarray [H,37]  -> matrix you feed the model (your multiplied matrix)\n",
    "      encoded: np.ndarray [H,37] -> original one-hot atom subtype matrix\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(path)\n",
    "    protein_id = basename[:4]\n",
    "    M_in = np.load(path)\n",
    "\n",
    "    one_hot_path = f\"/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-separate-graphs-5A/positive/{protein_id}-filtered_graphs.npy\"\n",
    "\n",
    "    obj = np.load(one_hot_path, allow_pickle=True).item()\n",
    "    encoded = obj[\"encoded_matrix\"]\n",
    "    return M_in, encoded\n",
    "\n",
    "\n",
    "def atom_type_from_onehot(onehot_row, type_names):\n",
    "    return type_names[int(np.argmax(onehot_row))] if np.any(onehot_row) else \"UNKNOWN\"\n",
    "\n",
    "def align_encoded_to_padded(M_in: np.ndarray, ENC: np.ndarray):\n",
    "    \"\"\"\n",
    "    Align a shorter, unpadded one-hot matrix (ENC: [N,37]) to a padded model input\n",
    "    (M_in: [H,37]) by placing ENC rows into the non-padded rows of M_in.\n",
    "\n",
    "    Returns:\n",
    "      ENC_aligned: [H,37] one-hot aligned/padded with zeros on padded rows\n",
    "      nonpad_mask: [H] boolean mask where True = real atom row\n",
    "    \"\"\"\n",
    "    assert M_in.ndim == 2 and M_in.shape[1] == 37, f\"M_in must be [H,37], got {M_in.shape}\"\n",
    "    assert ENC.ndim == 2 and ENC.shape[1] == 37,   f\"ENC must be [N,37], got {ENC.shape}\"\n",
    "\n",
    "    H = M_in.shape[0]\n",
    "    # Detect padded rows in M_in: all ~zero across 37 cols\n",
    "    nonpad_mask = ~np.isclose(M_in, 0.0, atol=1e-8).all(axis=1)\n",
    "    real_idx = np.flatnonzero(nonpad_mask)\n",
    "\n",
    "    ENC_aligned = np.zeros_like(M_in, dtype=np.float32)\n",
    "\n",
    "    if len(real_idx) == ENC.shape[0]:\n",
    "        # Best case: one-to-one—put ENC rows into the detected real rows\n",
    "        ENC_aligned[real_idx, :] = ENC\n",
    "    else:\n",
    "        # Fallback: assume ENC rows correspond to the first N rows\n",
    "        n = min(H, ENC.shape[0])\n",
    "        ENC_aligned[:n, :] = ENC[:n, :]\n",
    "        # Optional: warn if strong mismatch\n",
    "        if abs(len(real_idx) - ENC.shape[0]) > 0:\n",
    "            print(f\"[WARN] nonpad rows in M_in ({len(real_idx)}) != ENC rows ({ENC.shape[0]}). \"\n",
    "                  f\"Used first-{n} fallback alignment.\")\n",
    "\n",
    "    return ENC_aligned, nonpad_mask\n",
    "\n",
    "def save_bar_with_errorbars(values, errors, labels, out_path,\n",
    "                            title=\"\",\n",
    "                            ylabel=\"Percent of Top-K atoms\"):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    x = np.arange(len(values))\n",
    "    fig, ax = plt.subplots(figsize=(max(9, 0.35*len(values)), 5))\n",
    "    bars = ax.bar(x, values)\n",
    "    ax.errorbar(x, values, yerr=errors, fmt='none', capsize=4, linewidth=1)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, rotation=75, ha='right')\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_atom_residues(pdb_file):\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"mol\", pdb_file)\n",
    "    atoms = []\n",
    "\n",
    "    for model in structure:\n",
    "        for chain in model:\n",
    "            for residue in chain:\n",
    "                for atom in residue:\n",
    "                    if atom.is_disordered():\n",
    "                        # Add ALL conformers (A, B, etc.)\n",
    "                        for alt_atom in atom.disordered_get_list():\n",
    "                            atoms.append(alt_atom)\n",
    "                    else:\n",
    "                        atoms.append(atom)\n",
    "\n",
    "    residues = [\n",
    "        (\n",
    "            atom.get_parent().get_resname(),              # residue name (e.g., LEU)\n",
    "            atom.get_parent().get_id()[1],                # residue number\n",
    "            atom.get_parent().get_parent().get_id()       # chain ID\n",
    "        )\n",
    "        for atom in atoms\n",
    "    ]\n",
    "    return residues\n",
    "\n",
    "RES_ORDER_CANON = [\n",
    "    \"ALA\",\"ARG\",\"ASN\",\"ASP\",\"CYS\",\"GLN\",\"GLU\",\"GLY\",\"HIS\",\"ILE\",\n",
    "    \"LEU\",\"LYS\",\"MET\",\"PHE\",\"PRO\",\"SER\",\"THR\",\"TRP\",\"TYR\",\"VAL\"\n",
    "]\n",
    "\n",
    "def counts_to_percent(counts: Counter, total: int) -> dict:\n",
    "    if total <= 0:\n",
    "        return {k: 0.0 for k in counts}\n",
    "    return {k: (v/total)*100.0 for k, v in counts.items()}\n",
    "\n",
    "def order_residue_labels(all_labels):\n",
    "    # Start with canonical residues, then any others (alphabetical)\n",
    "    others = sorted([r for r in all_labels if r not in RES_ORDER_CANON])\n",
    "    ordered = [r for r in RES_ORDER_CANON if r in all_labels] + others\n",
    "    return ordered\n",
    "\n",
    "def write_residue_freq_csv(path, labels, counts_row, pct_row, total_k, k_per_sample):\n",
    "    # counts_row/pct_row are dicts keyed by residue label\n",
    "    import csv\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"residue\",\"count_in_topK\",\"percent_of_topK\",\"topk_atoms_total\",\"K_per_sample\"])\n",
    "        for r in labels:\n",
    "            w.writerow([r, int(counts_row.get(r,0)), float(pct_row.get(r,0.0)), int(total_k), int(k_per_sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb21d8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is exp index /home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy  is M_GLOB\n",
      "/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/4HQJ-filtered_combined_matrix.npy is first npy file\n",
      "[INFO] Preloaded 50 model(s).\n",
      "[DONE] Processed 57 positive sample(s). Outputs -> gnn_out_external/exp1\n",
      "1 is exp index /home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy  is M_GLOB\n",
      "/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/4HQJ-filtered_combined_matrix.npy is first npy file\n",
      "[INFO] Preloaded 50 model(s).\n",
      "[DONE] Processed 57 positive sample(s). Outputs -> gnn_out_external/exp2\n",
      "2 is exp index /home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy  is M_GLOB\n",
      "/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/4HQJ-filtered_combined_matrix.npy is first npy file\n",
      "[INFO] Preloaded 50 model(s).\n",
      "[DONE] Processed 57 positive sample(s). Outputs -> gnn_out_external/exp3\n",
      "3 is exp index /home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy  is M_GLOB\n",
      "/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/4HQJ-filtered_combined_matrix.npy is first npy file\n",
      "[INFO] Preloaded 50 model(s).\n",
      "[DONE] Processed 57 positive sample(s). Outputs -> gnn_out_external/exp4\n",
      "4 is exp index /home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy  is M_GLOB\n",
      "/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/4HQJ-filtered_combined_matrix.npy is first npy file\n",
      "[INFO] Preloaded 50 model(s).\n",
      "[DONE] Processed 57 positive sample(s). Outputs -> gnn_out_external/exp5\n"
     ]
    }
   ],
   "source": [
    "OUT_ROOT_BASE = r\"gnn_out_external\"\n",
    "os.makedirs(OUT_ROOT_BASE, exist_ok=True)\n",
    "\n",
    "# Collect per-experiment subtype % vectors (length W=37) for cross-experiment stats\n",
    "cross_exp_pct_list = []\n",
    "cross_exp_expnames = []\n",
    "\n",
    "for exp_index in range(5):\n",
    "    # if exp_index == 0:\n",
    "    #     M_GLOB = r\"/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/cholesterol-graph-5A/Test/Positive/*.npy\"\n",
    "    # else:\n",
    "    #     M_GLOB = f\"/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/cholesterol-graph-5A_exp{exp_index + 1}/Test/Positive/*.npy\"\n",
    "    M_GLOB = f\"/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/IvanTestSet/ivan-graph-5A/positive/*.npy\"\n",
    "    MODEL_GLOB   = f\"/home/alexhernandez/transmembranebindingAI/Models/Cholesterol/GNN/GNN-5A_Exp{exp_index + 1}/Models/*.pth\"\n",
    "    # ============================== CONFIG ===============================\n",
    "    OUT_ROOT = os.path.join(OUT_ROOT_BASE, f\"exp{exp_index+1}\")\n",
    "\n",
    "    print(exp_index, \"is exp index\", M_GLOB, \" is M_GLOB\")\n",
    "\n",
    "    # Your 37 atom-type names in the exact column order of your input matrices:\n",
    "    ATOM_TYPE_NAMES = [\n",
    "        'C', 'CA', 'CB', 'CD', 'CD1', 'CD2', 'CE', 'CE1', 'CE2', 'CE3', 'CG', 'CG1', 'CG2', 'CH2', 'CZ', 'CZ2', 'CZ3',\n",
    "        'O', 'OH', 'OD1', 'OD2', 'OE1', 'OE2', 'OG', 'OG1',\n",
    "        'N', 'NE', 'NE1', 'NE2', 'ND1', 'ND2', 'NZ', 'NH1', 'NH2',\n",
    "        'SD', 'SG'\n",
    "    ]\n",
    "    ATOM_TYPE_NAMES.append('UNKNOWN')  # makes 37 total\n",
    "\n",
    "    # Grad-CAM target: \"positive\" shows evidence FOR the positive class.\n",
    "    TARGET_CLASS = \"positive\"\n",
    "\n",
    "    # Safety: CPU-only is most stable. Set to False to try GPU with aggressive cleanup.\n",
    "    FORCE_CPU = True\n",
    "\n",
    "    if FORCE_CPU:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    DEVICE = torch.device(\"cuda\" if (not FORCE_CPU and torch.cuda.is_available()) else \"cpu\")\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    # ============================ MODEL & GRAD-CAM =========================\n",
    "    class CNN2D(nn.Module):\n",
    "        def __init__(self, in_ch=1, out_grid=(4,18)):\n",
    "            super().__init__()\n",
    "            self.conv1 = nn.Sequential(nn.Conv2d(in_ch,32,3,padding=1), nn.BatchNorm2d(32), nn.ReLU())\n",
    "            self.pool1 = nn.MaxPool2d(2,2)\n",
    "            self.conv2 = nn.Sequential(nn.Conv2d(32,64,3,padding=1), nn.BatchNorm2d(64), nn.ReLU())\n",
    "            self.pool2 = nn.MaxPool2d(2,2)\n",
    "            self.conv3 = nn.Sequential(nn.Conv2d(64,128,3,padding=1), nn.BatchNorm2d(128), nn.ReLU())\n",
    "            self.pool3 = nn.MaxPool2d(2,2)\n",
    "            self.adapt  = nn.AdaptiveAvgPool2d(out_grid)\n",
    "            self.flat   = nn.Flatten()\n",
    "            self.fc1    = nn.Linear(128*out_grid[0]*out_grid[1], 128)\n",
    "            self.drop   = nn.Dropout(0.5)\n",
    "            self.fc2    = nn.Linear(128, 1)\n",
    "        def forward(self, x):\n",
    "            x = self.pool1(self.conv1(x))\n",
    "            x = self.pool2(self.conv2(x))\n",
    "            x = self.pool3(self.conv3(x))\n",
    "            x = self.adapt(x)\n",
    "            x = self.flat(x)\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            x = self.drop(x)\n",
    "            return self.fc2(x)\n",
    "\n",
    "    class GradCAM:\n",
    "        def __init__(self, model, target_layer):\n",
    "            self.model, self.target_layer = model.eval(), target_layer\n",
    "            self.activations, self.gradients = None, None\n",
    "            self._fwd = target_layer.register_forward_hook(self._fwd_hook)\n",
    "            self._bwd = target_layer.register_full_backward_hook(self._bwd_hook)\n",
    "        def _fwd_hook(self, m,i,o): self.activations = o.detach()\n",
    "        def _bwd_hook(self, m,gi,go): self.gradients = go[0].detach()\n",
    "        @staticmethod\n",
    "        @torch.no_grad()\n",
    "        def _norm(cam):\n",
    "            B = cam.size(0)\n",
    "            flat = cam.view(B,-1)\n",
    "            flat = flat - flat.min(dim=1, keepdim=True).values\n",
    "            flat = flat / flat.max(dim=1, keepdim=True).values.clamp_min(1e-8)\n",
    "            return flat.view_as(cam)\n",
    "        def generate(self, x, target=\"positive\"):\n",
    "            B,C,H,W = x.shape\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            logits = self.model(x).squeeze(-1)           # [B]\n",
    "            score  = logits if target==\"positive\" else (-logits)\n",
    "            self.model.zero_grad(set_to_none=True)\n",
    "            score.sum().backward()\n",
    "            A, dA = self.activations, self.gradients\n",
    "            if A is None or dA is None:\n",
    "                raise RuntimeError(\"Grad-CAM hooks failed\")\n",
    "            w = dA.mean(dim=(2,3), keepdim=True)         # [B,C,1,1]\n",
    "            cam = (w*A).sum(dim=1, keepdim=True)         # [B,1,h,w]\n",
    "            cam = F.relu(cam)\n",
    "            if cam.shape[-2:] != (H,W):\n",
    "                cam = F.interpolate(cam, size=(H,W), mode='bilinear', align_corners=False)\n",
    "            return self._norm(cam), logits.detach()\n",
    "        def remove(self):\n",
    "            self._fwd.remove(); self._bwd.remove()\n",
    "\n",
    "    def last_conv2d(model):\n",
    "        lc = None\n",
    "        for m in model.modules():\n",
    "            if isinstance(m, nn.Conv2d): lc = m\n",
    "        return lc\n",
    "\n",
    "    def load_model(path, in_ch=1):\n",
    "        # weights_only=True removes the security warning and future-proofs your code\n",
    "        sd = torch.load(path, map_location=DEVICE, weights_only=True)\n",
    "        if isinstance(sd, dict) and \"state_dict\" in sd:\n",
    "            sd = sd[\"state_dict\"]\n",
    "        sd = { (k[7:] if k.startswith(\"module.\") else k): v for k,v in sd.items() }\n",
    "        m = CNN2D(in_ch=in_ch).to(DEVICE)\n",
    "        m.load_state_dict(sd, strict=True)\n",
    "        m.eval()\n",
    "        return m\n",
    "\n",
    "    # ========================= DISCOVER DATA/MODELS ========================\n",
    "    npy_files = sorted(glob.glob(M_GLOB))\n",
    "    print(npy_files[0], \"is first npy file\")\n",
    "    assert npy_files, f\"No .npy files match: {M_GLOB}\"\n",
    "\n",
    "    model_paths = sorted(glob.glob(MODEL_GLOB))\n",
    "\n",
    "    # Peek first sample using safe loader to get H,W\n",
    "    M0, ENC0 = load_sample(npy_files[0], (exp_index + 1))     # M0 -> model input, ENC0 -> original one-hot\n",
    "    assert M0.ndim == 2 and M0.shape[1] == 37, f\"Expected [H,37], got {M0.shape}\"\n",
    "    assert ENC0.ndim == 2 and ENC0.shape[1] == 37, f\"Expected ENC0 [N,37], got {ENC0.shape}\"\n",
    "\n",
    "    # Align the one-hot to the padded input\n",
    "    ENC0_aligned, nonpad0 = align_encoded_to_padded(M0, ENC0)\n",
    "\n",
    "    H, W = M0.shape\n",
    "    assert W == 37, f\"Expected 37 type columns, got {W}\"\n",
    "    assert len(ATOM_TYPE_NAMES) == 37, f\"ATOM_TYPE_NAMES must have 37 entries; got {len(ATOM_TYPE_NAMES)}\"\n",
    "\n",
    "    # ========================= PRELOAD MODELS & GRADCAM ====================\n",
    "    def preload_models(paths, in_ch=1):\n",
    "        \"\"\"Load all models once and attach persistent Grad-CAM hooks.\"\"\"\n",
    "        bank = []\n",
    "        for mp in paths:\n",
    "            try:\n",
    "                m = load_model(mp, in_ch=in_ch)     # already on DEVICE, eval(), strict load\n",
    "                conv = last_conv2d(m)\n",
    "                if conv is None:\n",
    "                    print(f\"[SKIP no conv2d] {os.path.basename(mp)}\")\n",
    "                    # release to be safe\n",
    "                    del m\n",
    "                    continue\n",
    "                gc_obj = GradCAM(m, conv)           # registers hooks\n",
    "                bank.append({\"path\": mp, \"model\": m, \"gc\": gc_obj})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[SKIP preload {os.path.basename(mp)}] {e}\")\n",
    "                try:\n",
    "                    del m, gc_obj\n",
    "                except:\n",
    "                    pass\n",
    "                gc.collect()\n",
    "                if DEVICE.type == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "        if not bank:\n",
    "            raise RuntimeError(\"No usable models after preload.\")\n",
    "        print(f\"[INFO] Preloaded {len(bank)} model(s).\")\n",
    "        return bank\n",
    "\n",
    "    models_bank = preload_models(model_paths, in_ch=1)\n",
    "\n",
    "    # ========================= AGGREGATE ACCUMULATORS ======================\n",
    "    sum_atom_importance = np.zeros((H,), dtype=np.float64)\n",
    "    sum_type_importance = np.zeros((W,), dtype=np.float64)\n",
    "    sum_cam             = np.zeros((H,W), dtype=np.float64)\n",
    "    count_samples       = 0\n",
    "    # Subtype frequency among Top-K atoms (aggregated over all samples)\n",
    "    topk_subtype_counts = np.zeros((W,), dtype=np.int64)\n",
    "    topk_total_atoms    = 0\n",
    "    topk_residue_counts = Counter()\n",
    "    topk_total_atoms_res = 0\n",
    "\n",
    "    # CSV for per-sample ensemble scores\n",
    "    scores_path = os.path.join(OUT_ROOT, \"per_sample_scores.csv\")\n",
    "    with open(scores_path, \"w\", newline=\"\") as fcsv:\n",
    "        csv.writer(fcsv).writerow([\"sample_name\",\"ensemble_prob_mean\",\"num_models_used\"])\n",
    "\n",
    "    # ========================= PROCESS EACH POSITIVE =======================\n",
    "    for sidx, fpath in enumerate(npy_files):\n",
    "        try:\n",
    "            # ---- load model input + original one-hot ----\n",
    "            M_in, ENC = load_sample(fpath, (exp_index + 1))      # both [H,37]\n",
    "            ENC_aligned, nonpad_mask = align_encoded_to_padded(M_in, ENC)\n",
    "\n",
    "            # ---- shape to model input [1,1,H,37] ----\n",
    "            x_np = M_in[None, None, :, :].astype(np.float32)  # [1,1,H,37]\n",
    "            x_t  = torch.from_numpy(x_np).to(DEVICE)\n",
    "\n",
    "            run_sum = np.zeros((H,37), dtype=np.float64)\n",
    "            logits_list = []\n",
    "            used = 0\n",
    "\n",
    "            # ---- look up residues from pdb ----\n",
    "            basename = os.path.basename(fpath)\n",
    "            protein_id = basename[:4]\n",
    "            residues = get_atom_residues(f\"/home/alexhernandez/transmembranebindingAI/Notebooks/Cholesterol/GNN/ivan-pdbs-distinct-5A/positive/{protein_id}-filtered.pdb\")\n",
    "\n",
    "            # --------- REUSE PRELOADED MODELS & HOOKS ---------\n",
    "            for entry in models_bank:\n",
    "                gc_obj = entry[\"gc\"]\n",
    "                try:\n",
    "                    cam_b1hw, logits = gc_obj.generate(x_t, target=TARGET_CLASS)\n",
    "                    cam = cam_b1hw[0,0].detach().cpu().numpy()  # [H,37]\n",
    "                    run_sum += cam\n",
    "                    logits_list.append(float(logits[0].item()))\n",
    "                    used += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"[SKIP {os.path.basename(entry['path'])} on {os.path.basename(fpath)}] {e}\")\n",
    "                finally:\n",
    "                    # Ensure no graph is held across iterations\n",
    "                    del cam_b1hw, logits\n",
    "                    gc.collect()\n",
    "                    if DEVICE.type == 'cuda':\n",
    "                        torch.cuda.empty_cache()\n",
    "\n",
    "            if used == 0:\n",
    "                print(f\"[WARN] {os.path.basename(fpath)} produced no CAMs; skipped.\")\n",
    "                continue\n",
    "\n",
    "            # Average across models, then (optionally) renormalize to [0,1] for visuals\n",
    "            cam_mean = (run_sum / used).astype(np.float32)     # [H,37]\n",
    "            cam_mean_vis = normalize01(cam_mean)               # for images only\n",
    "\n",
    "            mean_logit = float(np.mean(logits_list))\n",
    "            prob = float(1/(1+np.exp(-mean_logit)))\n",
    "\n",
    "            # ===================== IMPORTANCE LOGIC (unchanged) =====================\n",
    "            atom_importance = cam_mean.sum(axis=1).astype(np.float32)   # [H]\n",
    "            type_importance = cam_mean.sum(axis=0).astype(np.float32)   # [37]\n",
    "\n",
    "            # ===================== OUTPUTS (unchanged) =====================\n",
    "            sname = os.path.splitext(os.path.basename(fpath))[0]\n",
    "            sdir  = os.path.join(OUT_ROOT, f\"sample_{sidx:05d}_{sname}\")\n",
    "            os.makedirs(sdir, exist_ok=True)\n",
    "\n",
    "            np.save(os.path.join(sdir, \"input_M.npy\"), M_in)\n",
    "            np.save(os.path.join(sdir, \"encoded_matrix.npy\"), ENC)\n",
    "            np.save(os.path.join(sdir, \"ensemble_cam.npy\"), cam_mean)\n",
    "            np.save(os.path.join(sdir, \"atom_importance.npy\"), atom_importance)\n",
    "            np.save(os.path.join(sdir, \"type_importance.npy\"), type_importance)\n",
    "\n",
    "            # atoms CSV\n",
    "            atoms_csv = os.path.join(sdir, \"atoms_importance.csv\")\n",
    "            with open(atoms_csv, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"atom_index\",\"is_real_atom\",\"atom_type_from_onehot\",\"importance\"])\n",
    "                for a in range(H):\n",
    "                    is_real = int(nonpad_mask[a])\n",
    "                    if not is_real:\n",
    "                        continue\n",
    "                    atom_type = ATOM_TYPE_NAMES[int(np.argmax(ENC_aligned[a]))] if np.any(ENC_aligned[a]) else \"UNKNOWN\"\n",
    "                    w.writerow([a, is_real, atom_type, float(atom_importance[a])])\n",
    "\n",
    "            # per-type CSV\n",
    "            types_csv = os.path.join(sdir, \"type_importance.csv\")\n",
    "            with open(types_csv, \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"atom_type\",\"importance_sum\"])\n",
    "                for j in range(37):\n",
    "                    w.writerow([ATOM_TYPE_NAMES[j], float(type_importance[j])])\n",
    "\n",
    "            # Visuals (unchanged)\n",
    "            save_png_gray(M_in,                    os.path.join(sdir, \"input_M.png\"))\n",
    "            save_png_gray(cam_mean_vis,            os.path.join(sdir, \"ensemble_cam.png\"))\n",
    "            save_overlay(M_in, cam_mean_vis,       os.path.join(sdir, \"ensemble_cam_overlay.png\"))\n",
    "\n",
    "            save_line_plot_pil(atom_importance,    os.path.join(sdir, \"atom_importance_sum_plot.png\"),\n",
    "                            title=\"Atom importance (SUM over type-columns)\",\n",
    "                            xlabel=\"Atom index\", ylabel=\"Importance (sum)\")\n",
    "\n",
    "            # Top-K atoms (respecting nonpad)\n",
    "            valid_rows_mask = nonpad_mask & (ENC_aligned.sum(axis=1) > 0)\n",
    "            valid_idx = np.flatnonzero(valid_rows_mask)\n",
    "            if valid_idx.size > 0:\n",
    "                order_local = np.argsort(atom_importance[valid_idx])[::-1]\n",
    "                k_local = min(5, order_local.size)\n",
    "                top_rows = valid_idx[order_local[:k_local]]\n",
    "                top_vals   = atom_importance[top_rows]\n",
    "                top_labels = [f\"{idx} ({ATOM_TYPE_NAMES[int(np.argmax(ENC_aligned[idx]))]})\" for idx in top_rows]\n",
    "                save_bar_plot_pil(top_vals, top_labels, os.path.join(sdir, \"top_atoms_bar.png\"),\n",
    "                                title=f\"Top {k_local} atoms by importance (SUM across types)\")\n",
    "\n",
    "                # ---- Global Top-K subtype frequency updates ----\n",
    "                subtypes_top = np.argmax(ENC_aligned[top_rows], axis=1)\n",
    "                np.add.at(topk_subtype_counts, subtypes_top, 1)\n",
    "                topk_total_atoms += k_local\n",
    "\n",
    "            # append sample score\n",
    "            with open(scores_path, \"a\", newline=\"\") as fcsv:\n",
    "                csv.writer(fcsv).writerow([sname, prob, used])\n",
    "\n",
    "            # aggregates\n",
    "            sum_atom_importance += atom_importance\n",
    "            sum_type_importance += type_importance\n",
    "            sum_cam             += cam_mean\n",
    "            count_samples       += 1\n",
    "            # ---- Residue frequency updates for this experiment (Top-K atoms) ----\n",
    "            # SAFETY: ensure residues array is aligned to your real atom rows.\n",
    "            # A good check: assert len(residues) == int(nonpad_mask.sum()).\n",
    "            try:\n",
    "                assert len(residues) == int(nonpad_mask.sum()), \\\n",
    "                    f\"Residue list ({len(residues)}) != real atoms ({int(nonpad_mask.sum())}). Check alignment.\"\n",
    "            except AssertionError as _e:\n",
    "                # You can print a warning and skip residue counting for this sample if misaligned.\n",
    "                print(\"[WARN residues alignment]\", _e)\n",
    "            else:\n",
    "                # Map padded row index -> compact real-atom index\n",
    "                # This relies on your padding scheme being \"left aligned\" with nonpad rows in order.\n",
    "                # If your aligner reorders, replace the map with a true index map from align_encoded_to_padded.\n",
    "                real_row_indices = np.flatnonzero(nonpad_mask)  # positions of real atoms in padded matrix\n",
    "                padded_to_real = {p_idx: r_idx for r_idx, p_idx in enumerate(real_row_indices)}\n",
    "\n",
    "                # Collect residue names for Top-K rows\n",
    "                for pr in top_rows:\n",
    "                    if pr in padded_to_real:\n",
    "                        rr = padded_to_real[pr]          # 0..(num_real_atoms-1)\n",
    "                        resname, resnum, chain = residues[rr]\n",
    "                        topk_residue_counts[resname] += 1\n",
    "                        topk_total_atoms_res += 1\n",
    "                    else:\n",
    "                        # Shouldn't happen if top_rows ⊆ real rows; keep for safety\n",
    "                        pass\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL sample {fpath}] {e}\")\n",
    "\n",
    "    # ============================== CLEANUP =================================\n",
    "    # Remove hooks and free models after all samples\n",
    "    for entry in models_bank:\n",
    "        try:\n",
    "            entry[\"gc\"].remove()\n",
    "        except Exception:\n",
    "            pass\n",
    "        # keep explicit deletes to encourage early free\n",
    "        del entry[\"gc\"]\n",
    "        del entry[\"model\"]\n",
    "    gc.collect()\n",
    "    if DEVICE.type == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ============================= AGGREGATES ==============================\n",
    "    if count_samples > 0:\n",
    "        agg_dir = os.path.join(OUT_ROOT, \"aggregate\")\n",
    "        os.makedirs(agg_dir, exist_ok=True)\n",
    "\n",
    "        mean_atom_importance = (sum_atom_importance / count_samples).astype(np.float32)  # [H]\n",
    "        mean_type_importance = (sum_type_importance / count_samples).astype(np.float32)  # [37]\n",
    "        mean_cam             = (sum_cam / count_samples).astype(np.float32)             # [H,37]\n",
    "\n",
    "        # arrays\n",
    "        np.save(os.path.join(agg_dir, \"mean_atom_importance.npy\"), mean_atom_importance)\n",
    "        np.save(os.path.join(agg_dir, \"mean_type_importance.npy\"), mean_type_importance)\n",
    "        np.save(os.path.join(agg_dir, \"mean_cam.npy\"),             mean_cam)\n",
    "\n",
    "        # CSVs\n",
    "        with open(os.path.join(agg_dir, \"mean_atom_importance.csv\"), \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f); w.writerow([\"atom_index\",\"mean_importance\"])\n",
    "            for a in range(H): w.writerow([a, float(mean_atom_importance[a])])\n",
    "\n",
    "        with open(os.path.join(agg_dir, \"mean_type_importance.csv\"), \"w\", newline=\"\") as f:\n",
    "            w = csv.writer(f); w.writerow([\"atom_type\",\"mean_importance\"])\n",
    "            for j in range(37): w.writerow([ATOM_TYPE_NAMES[j], float(mean_type_importance[j])])\n",
    "\n",
    "        # Top-K subtype frequency CSV/plot (unchanged)\n",
    "        if topk_total_atoms > 0:\n",
    "            freq = topk_subtype_counts.astype(np.int64)\n",
    "            pct  = (freq / topk_total_atoms) * 100.0\n",
    "            with open(os.path.join(agg_dir, \"subtype_topk_frequency.csv\"), \"w\", newline=\"\") as f:\n",
    "                w = csv.writer(f)\n",
    "                w.writerow([\"atom_type\", \"count_in_topK\", \"percent_of_topK\", \"topk_atoms_total\", \"K_per_sample\"])\n",
    "                for j in range(W):\n",
    "                    w.writerow([ATOM_TYPE_NAMES[j], int(freq[j]), float(pct[j]), int(topk_total_atoms), int(5)])\n",
    "\n",
    "            order = np.argsort(freq)[::-1]\n",
    "            order = [j for j in order if freq[j] > 0]\n",
    "            if order:\n",
    "                save_bar_plot_pil(\n",
    "                    freq[order],\n",
    "                    [ATOM_TYPE_NAMES[j] for j in order],\n",
    "                    os.path.join(agg_dir, \"top_subtypes_topk_frequency_bar.png\"),\n",
    "                    title=f\"Subtype frequency among Top-K atoms (total counted: {topk_total_atoms})\"\n",
    "                )\n",
    "            # ---- Save this experiment's percent vector for cross-experiment stats ----\n",
    "            cross_exp_pct_list.append(pct.copy())              # pct is length W\n",
    "            cross_exp_expnames.append(f\"exp{exp_index+1}\")\n",
    "\n",
    "        # Keep this experiment's raw counts (length W) for cross-experiment counts table\n",
    "        if topk_total_atoms > 0:\n",
    "            if 'cross_exp_freq_list' not in globals():\n",
    "                cross_exp_freq_list = []\n",
    "                cross_exp_expnames_counts = []\n",
    "            cross_exp_freq_list.append(freq.copy())                          # raw counts per subtype (length W)\n",
    "            cross_exp_expnames_counts.append(f\"exp{exp_index+1}\")\n",
    "        \n",
    "        # -------- Per-experiment RESIDUE frequency outputs --------\n",
    "        if topk_total_atoms_res > 0:\n",
    "            # Normalize to percent\n",
    "            res_pct = counts_to_percent(topk_residue_counts, topk_total_atoms_res)\n",
    "\n",
    "            # Choose a stable label order across experiments: union with canonical preference\n",
    "            exp_res_labels = order_residue_labels(list(topk_residue_counts.keys()))\n",
    "\n",
    "            # Save per-experiment residues CSV\n",
    "            residues_csv = os.path.join(agg_dir, \"residues_topk_frequency.csv\")\n",
    "            write_residue_freq_csv(\n",
    "                residues_csv,\n",
    "                exp_res_labels,\n",
    "                topk_residue_counts,\n",
    "                res_pct,\n",
    "                topk_total_atoms_res,\n",
    "                5  # your K per-sample (adjust if you changed it)\n",
    "            )\n",
    "\n",
    "            # Save per-experiment bar (counts). You can also plot % if you prefer.\n",
    "            # Reuse your save_bar_plot_pil helper; if it expects numpy arrays, cast as needed.\n",
    "            from numpy import array as nparr\n",
    "            save_bar_plot_pil(\n",
    "                nparr([topk_residue_counts[r] for r in exp_res_labels]),\n",
    "                exp_res_labels,\n",
    "                os.path.join(agg_dir, \"residues_topk_frequency_bar.png\"),\n",
    "                title=f\"Residue frequency among Top-K atoms (total counted: {topk_total_atoms_res})\"\n",
    "            )\n",
    "\n",
    "            # ---- Collect for cross-experiment mean±SD ----\n",
    "            # We collect as a dense vector aligned to a global label order later.\n",
    "            if 'cross_exp_residue_pct_list' not in globals():\n",
    "                cross_exp_residue_pct_list = []\n",
    "                cross_exp_residue_counts_list = []\n",
    "                cross_exp_residue_labels_union = set()\n",
    "                cross_exp_residue_expnames = []\n",
    "\n",
    "            cross_exp_residue_pct_list.append(res_pct)           # dict: residue -> %\n",
    "            cross_exp_residue_counts_list.append(topk_residue_counts.copy())\n",
    "            cross_exp_residue_labels_union.update(res_pct.keys())\n",
    "            cross_exp_residue_expnames.append(f\"exp{exp_index+1}\")\n",
    "\n",
    "\n",
    "\n",
    "        # visuals\n",
    "        save_line_plot_pil(mean_atom_importance, os.path.join(agg_dir, \"mean_atom_importance_plot.png\"),\n",
    "                        title=\"Mean atom importance (positives)\", xlabel=\"Atom index\", ylabel=\"Mean importance\")\n",
    "        order = np.argsort(mean_type_importance)[::-1]\n",
    "        save_bar_plot_pil(mean_type_importance[order], [ATOM_TYPE_NAMES[i] for i in order],\n",
    "                        os.path.join(agg_dir, \"mean_type_importance_bar.png\"),\n",
    "                        title=\"Mean atom-type importance (positives)\")\n",
    "        save_png_gray(mean_cam, os.path.join(agg_dir, \"mean_cam.png\"))\n",
    "\n",
    "    print(f\"[DONE] Processed {count_samples} positive sample(s). Outputs -> {OUT_ROOT}\")\n",
    "\n",
    "# =================== CROSS-EXPERIMENT: COUNTS TABLE (Top-K) ===================\n",
    "# Creates a CSV like the example:\n",
    "# __experiment__,C,N,O,SD,CE,CZ3,CH2,CD1,CG2,CB,OG1,CA,CZ,OH,NH1,NE,NH2,SG,OG,CE2,CE3,CD2,CG,OD1,ND2,NE1,CE1,CG1,NE2,OD2,CD,OE1,CZ2,ND1\n",
    "if 'cross_exp_freq_list' in globals() and cross_exp_freq_list:\n",
    "    exp_freqs = np.stack(cross_exp_freq_list, axis=0)  # [E, W]\n",
    "    E, W_local = exp_freqs.shape\n",
    "    assert W_local == len(ATOM_TYPE_NAMES), \"Mismatch in subtype dimension.\"\n",
    "\n",
    "    # Map atom type -> column index in working vectors\n",
    "    type_to_idx = {name: idx for idx, name in enumerate(ATOM_TYPE_NAMES)}\n",
    "\n",
    "    cross_dir = os.path.join(OUT_ROOT_BASE, \"cross_experiment\")\n",
    "    os.makedirs(cross_dir, exist_ok=True)\n",
    "\n",
    "    out_counts_csv = os.path.join(cross_dir, \"top_subtypes_topk_frequency_counts_by_experiment.csv\")\n",
    "    with open(out_counts_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"__experiment__\"] + ATOM_TYPE_NAMES)\n",
    "\n",
    "        for e, exp_name in enumerate(cross_exp_expnames_counts):\n",
    "            row = [exp_name]\n",
    "            for t in ATOM_TYPE_NAMES:\n",
    "                idx = type_to_idx.get(t, None)\n",
    "                val = int(exp_freqs[e, idx]) if idx is not None else 0\n",
    "                row.append(val)\n",
    "            w.writerow(row)\n",
    "\n",
    "# ======================= CROSS-EXPERIMENT RESIDUE AGG =======================\n",
    "if 'cross_exp_residue_pct_list' in globals() and cross_exp_residue_pct_list:\n",
    "    cross_dir = os.path.join(OUT_ROOT_BASE, \"cross_experiment\")\n",
    "    os.makedirs(cross_dir, exist_ok=True)\n",
    "\n",
    "    # Create a stable global order for residues (canonical-first, then others)\n",
    "    all_residues = order_residue_labels(list(cross_exp_residue_labels_union))\n",
    "\n",
    "    # Build dense matrix [E, R] of percents in this order\n",
    "    import numpy as np, csv\n",
    "    E = len(cross_exp_residue_pct_list)\n",
    "    R = len(all_residues)\n",
    "    pct_mat = np.zeros((E, R), dtype=float)\n",
    "\n",
    "    for e, pct_dict in enumerate(cross_exp_residue_pct_list):\n",
    "        for j, res in enumerate(all_residues):\n",
    "            pct_mat[e, j] = float(pct_dict.get(res, 0.0))\n",
    "\n",
    "    mean_pct = pct_mat.mean(axis=0)\n",
    "    std_pct  = pct_mat.std(axis=0, ddof=1) if E > 1 else np.zeros_like(mean_pct)\n",
    "\n",
    "    # CSV: mean/std plus per-experiment columns\n",
    "    csv_path = os.path.join(cross_dir, \"top_residues_topk_frequency_mean_std.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"residue\", \"mean_percent\", \"std_percent\"] + [f\"{name}_percent\" for name in cross_exp_residue_expnames]\n",
    "        w.writerow(header)\n",
    "        for j, res in enumerate(all_residues):\n",
    "            row = [res, float(mean_pct[j]), float(std_pct[j])] + [float(pct_mat[e, j]) for e in range(E)]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # Plot mean ± SD (reuse your errorbar helper)\n",
    "    order = np.argsort(mean_pct)[::-1]\n",
    "    order = [idx for idx in order if (mean_pct[idx] > 0) or (std_pct[idx] > 0)]\n",
    "    if order:\n",
    "        png_path = os.path.join(cross_dir, \"top_residues_topk_frequency_mean_std.png\")\n",
    "        save_bar_with_errorbars(\n",
    "            values=mean_pct[order],\n",
    "            errors=std_pct[order],\n",
    "            labels=[all_residues[j] for j in order],\n",
    "            out_path=png_path,\n",
    "            title=f\"Residue frequency among Top-K atoms (mean ± SD across {E} experiments)\",\n",
    "            ylabel=\"Percent of Top-K atoms\"\n",
    "        )\n",
    "\n",
    "    # (Optional) also save cross-experiment COUNTS table aligned by residues\n",
    "    # Build counts matrix [E, R]\n",
    "    cnt_mat = np.zeros((E, R), dtype=int)\n",
    "    for e, cnts in enumerate(cross_exp_residue_counts_list):\n",
    "        for j, res in enumerate(all_residues):\n",
    "            cnt_mat[e, j] = int(cnts.get(res, 0))\n",
    "\n",
    "    out_counts_csv = os.path.join(cross_dir, \"top_residues_topk_frequency_counts_by_experiment.csv\")\n",
    "    with open(out_counts_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"__experiment__\"] + all_residues)\n",
    "        for e, name in enumerate(cross_exp_residue_expnames):\n",
    "            w.writerow([name] + [int(cnt_mat[e, j]) for j in range(R)])\n",
    "\n",
    "# ======================= CROSS-EXPERIMENT AGGREGATION =======================\n",
    "if cross_exp_pct_list:\n",
    "    exp_pcts = np.stack(cross_exp_pct_list, axis=0)  # [E, W]\n",
    "    E, W = exp_pcts.shape\n",
    "\n",
    "    mean_pct = exp_pcts.mean(axis=0)                       # [W]\n",
    "    std_pct  = exp_pcts.std(axis=0, ddof=1) if E > 1 else np.zeros_like(mean_pct)\n",
    "\n",
    "    # Order by mean descending, keep only those with any signal\n",
    "    order = np.argsort(mean_pct)[::-1]\n",
    "    order = [j for j in order if mean_pct[j] > 0 or std_pct[j] > 0]\n",
    "\n",
    "    cross_dir = os.path.join(OUT_ROOT_BASE, \"cross_experiment\")\n",
    "    os.makedirs(cross_dir, exist_ok=True)\n",
    "\n",
    "    # CSV: mean, std, and per-experiment columns\n",
    "    csv_path = os.path.join(cross_dir, \"top_subtypes_topk_frequency_mean_std.csv\")\n",
    "    with open(csv_path, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        header = [\"atom_type\", \"mean_percent\", \"std_percent\"] + [f\"{name}_percent\" for name in cross_exp_expnames]\n",
    "        w.writerow(header)\n",
    "        for j in range(W):\n",
    "            row = [ATOM_TYPE_NAMES[j], float(mean_pct[j]), float(std_pct[j])] + [float(exp_pcts[e, j]) for e in range(E)]\n",
    "            w.writerow(row)\n",
    "\n",
    "    # Plot (mean ± std) as error bars\n",
    "    if order:\n",
    "        png_path = os.path.join(cross_dir, \"top_subtypes_topk_frequency_mean_std.png\")\n",
    "        save_bar_with_errorbars(\n",
    "            values=mean_pct[order],\n",
    "            errors=std_pct[order],\n",
    "            labels=[ATOM_TYPE_NAMES[j] for j in order],\n",
    "            out_path=png_path,\n",
    "            title=f\"Subtype frequency among Top-K atoms (mean ± SD across {E} experiments)\",\n",
    "            ylabel=\"Percent of Top-K atoms\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38d73a0-0c6c-4f77-8d11-4d7ee06a4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def min_max_normalization(matrix):\n",
    "#     \"\"\"\n",
    "#     Perform Min-Max normalization on a given matrix.\n",
    "\n",
    "#     Parameters:\n",
    "#     matrix (np.ndarray): The input matrix to be normalized.\n",
    "\n",
    "#     Returns:\n",
    "#     np.ndarray: The normalized matrix with values scaled to the range [0, 1].\n",
    "#     \"\"\"\n",
    "#     # Compute the minimum and maximum values for the matrix\n",
    "#     min_val = np.min(matrix)\n",
    "#     max_val = np.max(matrix)\n",
    "\n",
    "#     # Apply Min-Max normalization formula\n",
    "#     normalized_matrix = (matrix - min_val) / (max_val - min_val)\n",
    "\n",
    "#     return normalized_matrix\n",
    "\n",
    "# data = np.load(\"/home/alexhernandez/transmembranebindingAI/Data/SplitData/Cholesterol/cholesterol-separate-graphs-5A_exp1/Positive/1LRI-filtered_graphs.npy\", allow_pickle=True).item()\n",
    "# inverse_distance = data['inverse_distance']\n",
    "# encoded_matrix = data['encoded_matrix']\n",
    "\n",
    "# print(encoded_matrix[:10])\n",
    "# print(inverse_distance[:10])\n",
    "\n",
    "# max_atoms = 150\n",
    "\n",
    "# combined_matrix = inverse_distance @ encoded_matrix # for gnn\n",
    "# combined_matrix = min_max_normalization(combined_matrix)\n",
    "\n",
    "# num_atoms = inverse_distance.shape[0]\n",
    "\n",
    "# combined_matrix = np.pad(combined_matrix, ((0, max_atoms - num_atoms), (0, 0)), mode='constant') # padding for gnn\n",
    "# print(combined_matrix[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
