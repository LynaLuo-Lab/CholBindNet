{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 3D CNN Model\n",
    "class CNN3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN3D, self).__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv3d(in_channels=23, out_channels=64, kernel_size=1, stride=1, padding=0) # play around with output channels\n",
    "        self.conv1 = nn.Conv3d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "        self.conv2 = nn.Conv3d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        #self.dropout_conv = nn.Dropout3d(p=0.05)\n",
    "        \n",
    "        # After two pooling layers, spatial dimensions reduce from 40x40x40 -> 5x5x5\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3 * 3, 256)  # Try increasing over 256\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)  # Assuming 1 output for docking status/position\n",
    "\n",
    "        #self.dropout_fc = nn.Dropout(p=0.15)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass through Conv layers\n",
    "        x = self.pool(torch.relu(self.conv0(x)))  # Conv0 -> ReLU -> Pooling\n",
    "        #x = self.dropout_conv(x)\n",
    "        x = self.pool(torch.relu(self.conv1(x)))  # Conv1 -> ReLU -> Pooling\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))  # Conv2 -> ReLU -> Pooling\n",
    "\n",
    "        # Flatten the input for fully connected layers\n",
    "        x = x.view(-1, 128 * 3 * 3 * 3)\n",
    "        \n",
    "        # Forward pass through fully connected layers\n",
    "        x = torch.relu(self.fc1(x)) #use tanh activation\n",
    "        #x = self.dropout_fc(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.nn.functional.softmax(self.fc3(x), dim=1)  # Final layer (output layer)\n",
    "        #x = torch.clamp(x, min=1e-7, max=1 - 1e-7)  # Clamp outputs to avoid extreme values\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridDataset(Dataset):\n",
    "    def __init__(self, data_dict):\n",
    "        self.data = list(data_dict.values())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        grid = sample['grid_tensor']\n",
    "        label = sample['label']\n",
    "        return grid, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3.0)\n",
    "\n",
    "        total_grad_norm = 0\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                total_grad_norm += param.grad.norm().item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    return (running_loss / len(dataloader)), total_grad_norm\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    validation_loss = running_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    return validation_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925 is length of positive\n",
      "In validation directory there are 385 positives and 1385 fragments\n",
      "1770 is length of validation grids\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with file names as keys and label + tensor grid as values\n",
    "positive_grids = glob.glob('../../../Data/SplitData/Cholesterol/cholesterol-grid-5A_exp3/PositiveWithoutSpies/*.npy')\n",
    "validation_grids = glob.glob('../../../Data/SplitData/Cholesterol/cholesterol-grid-5A_exp3/Validation_Set/*.npy')\n",
    "file_data = {} # format is filename as key, label and grid tensor are values\n",
    "\n",
    "for file in positive_grids:\n",
    "    # Load the numpy array and convert it to a PyTorch tensor\n",
    "    grid = np.load(file)\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32).permute(3, 0, 1, 2)  # Fix input format\n",
    "    file_data[file] = {'label': 1, 'grid_tensor': grid_tensor}\n",
    "positive_grids = file_data\n",
    "print(len(positive_grids), \"is length of positive\")\n",
    "\n",
    "file_data = {} # format is filename as key, label and grid tensor are values\n",
    "\n",
    "positive_validation_count = 0\n",
    "unlabeled_validation_count = 0\n",
    "\n",
    "for file in validation_grids:\n",
    "    # Load the numpy array and convert it to a PyTorch tensor\n",
    "    grid = np.load(file)\n",
    "    grid_tensor = torch.tensor(grid, dtype=torch.float32).permute(3, 0, 1, 2)  # Fix input format\n",
    "    if any(f\"-f{i}\" in file for i in range(1, 6)):\n",
    "        label = 0\n",
    "        unlabeled_validation_count += 1\n",
    "    else:\n",
    "        label = 1\n",
    "        positive_validation_count += 1\n",
    "\n",
    "    file_data[file] = {'label': label, 'grid_tensor': grid_tensor}\n",
    "print(\"In validation directory there are\", positive_validation_count, \"positives and\", unlabeled_validation_count, \"fragments\")\n",
    "validation_grids = file_data\n",
    "print(len(validation_grids), \"is length of validation grids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(train_losses, validation_losses, validation_accuracies, learning_rates):\n",
    "    # Plot Training Loss vs Validation Loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "    plt.plot(range(1, len(validation_losses) + 1), validation_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss vs Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Validation Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(validation_accuracies) + 1), validation_accuracies, label='Validation Accuracy', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Learning Rate\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(1, len(learning_rates) + 1), learning_rates, label='Learning Rates', color='green')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Validation Accuracy over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1925 is length of subset grid\n",
      "Training on bin 1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 3.30 GiB. GPU 0 has a total capacity of 19.70 GiB of which 347.62 MiB is free. Process 3308779 has 14.63 GiB memory in use. Including non-PyTorch memory, this process has 4.71 GiB memory in use. Of the allocated memory 4.49 GiB is allocated by PyTorch, and 18.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m learning_rates \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 51\u001b[0m     epoch_loss, grad_norm \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     validation_loss, validation_accuracy \u001b[38;5;241m=\u001b[39m validate_model(model, validation_dataloader, criterion, device)\n\u001b[1;32m     53\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep(validation_loss)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cpp-ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cpp-ml/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[1], line 32\u001b[0m, in \u001b[0;36mCNN3D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Forward pass through Conv layers\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Conv0 -> ReLU -> Pooling\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#x = self.dropout_conv(x)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))  \u001b[38;5;66;03m# Conv1 -> ReLU -> Pooling\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.30 GiB. GPU 0 has a total capacity of 19.70 GiB of which 347.62 MiB is free. Process 3308779 has 14.63 GiB memory in use. Including non-PyTorch memory, this process has 4.71 GiB memory in use. Of the allocated memory 4.49 GiB is allocated by PyTorch, and 18.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Training loop\n",
    "epochs = 150\n",
    "batch_size = 512\n",
    "\n",
    "validation_dataset = GridDataset(validation_grids)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for i in range(0, 50):\n",
    "    k = i + 1\n",
    "    file_data = {}\n",
    "    subset_grid = glob.glob(f'../../../Data/SplitData/Cholesterol/cholesterol-grid-5A_exp3/k_subsets/subset_{k}/*.npy')  # Adjust path as needed\n",
    "    for file in subset_grid:\n",
    "        # Load the numpy array and convert it to a PyTorch tensor\n",
    "        grid = np.load(file)\n",
    "        grid_tensor = torch.tensor(grid, dtype=torch.float32).permute(3, 0, 1, 2)  # Fix input format\n",
    "        file_data[file] = {'label': 0, 'grid_tensor': grid_tensor} # 0 means unlabeled\n",
    "    subset_grid = file_data\n",
    "    print(len(subset_grid), \"is length of subset grid\")\n",
    "    \n",
    "    bin = {**positive_grids, **subset_grid} # merged\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # specify to run on 3 gpu, run on scrum, convert to .py \n",
    "    \n",
    "    # Initialize the model\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define paths for saving models\n",
    "    save_dir = \"3DCholesterolModels-5A_exp3\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    model = CNN3D().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.00005, weight_decay=1e-4) # maybe try momentum 0.9\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=1e-4)\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.90)  # Reduce LR every 10 epochs\n",
    "    #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.90)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5000, eta_min=1e-6)\n",
    "    \n",
    "    print(f\"Training on bin {k}\")\n",
    "    dataset = GridDataset(bin)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    validation_accuracies = []\n",
    "    learning_rates = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss, grad_norm = train_model(model, dataloader, criterion, optimizer, device)\n",
    "        validation_loss, validation_accuracy = validate_model(model, validation_dataloader, criterion, device)\n",
    "        scheduler.step(validation_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr'] \n",
    "        train_losses.append(epoch_loss)\n",
    "        validation_losses.append(validation_loss)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "        learning_rates.append(current_lr)\n",
    "    \n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Bin {k}, Epoch {epoch+1}/{epochs}, \"\n",
    "                f\"Train Loss: {epoch_loss:.4f}, Validation Loss: {validation_loss:.4f}, \"\n",
    "                f\"Validation Accuracy: {validation_accuracy:.4f}, Grad Norm: {grad_norm:.4f}, \"\n",
    "                f\"LR: {current_lr:.6f}\"\n",
    "            )\n",
    "            \n",
    "    plot_graphs(train_losses, validation_losses, validation_accuracies, learning_rates)\n",
    "    \n",
    "    #Save the trained model\n",
    "    model_path = os.path.join(save_dir, f\"model_bin_{k}.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model for bin {k} saved to {model_path}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpp-ml",
   "language": "python",
   "name": "cpp-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
